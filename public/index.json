[{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nCalifornia Housing Dataset\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ h_{\\theta}(x^{(i)}) = \\theta_{1} x_1^{(i)} + \\theta_{2} x_2^{(i)} + \\theta_{3} x_3^{(i)} + \\dots + \\theta_{d} x_d^{(i)} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n$x^{(i)}$ is of the shape $(1,d)$ and $θ$ is of the shape $(d,1)$ (you can see that the shapes are compatible and you can do matrix multiplication)\n$h_θ(x^{(i)})$ is of the shape $(1,1)$ which is nothing but a scalar and it is what you expect because it is the price of a single house.\nNow, we can stack all the examples (or rows) together and write it in a compact matrix form like,\n$$ \\hat{y} = h_{\\theta}(X) = X \\theta $$\n$X$ is of the shape $(n,d)$ and $θ$ is of the shape $(d,1)$.\n$h_θ(X)$ and $\\hat{y}$ are of the shape $(n,1)$ which are the prices of $n$ houses stored in a vector.\nLoss Function From the above discussion, you must\u0026rsquo;ve figured out that the goal is to find the right value of $\\theta$. That is, the $\\theta$ which gives the correct value or value as close as possible to $y$, given $X$.\nTherefore, you can say that if you\u0026rsquo;re predicted vector $\\hat{y}$ is close to the actual vector $y$, then you are proceding in the right direction with respect to $\\theta$.\nSo, the goal is to minimize the difference between $y$ and $\\hat{y}$.\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n |\\hat{y}^{(i)} - y^{(i)}| $$\nwhere:\n$\\theta^{*}$ is the optimal(right) value of $\\theta$. $\\hat{y} = X\\theta$ represents the predicted values $y$ is the actual output vector The term $|\\hat{y}^{(i)} - y^{(i)}|$ represents the represents the error (or residual) for each data point. Modulus ensures that all errors are positive, so that underestimations and overestimations don’t cancel each other out. But, in practice, we DO NOT use the above equation. We use,\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$\nThis is because:\nMathematical convenience: The squared function is differentiable everywhere, making it easier to differentiate and compute derivatives compared to the modulus function which is not differentiable at it\u0026rsquo;s minimum. Square function vs modulus function\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nCalifornia Housing Dataset\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ h_{\\theta}(x^{(i)}) = \\theta_{1} x_1^{(i)} + \\theta_{2} x_2^{(i)} + \\theta_{3} x_3^{(i)} + \\dots + \\theta_{d} x_d^{(i)} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n$x^{(i)}$ is of the shape $(1,d)$ and $θ$ is of the shape $(d,1)$ (you can see that the shapes are compatible and you can do matrix multiplication)\n$h_θ(x^{(i)})$ is of the shape $(1,1)$ which is nothing but a scalar and it is what you expect because it is the price of a single house.\nNow, we can stack all the examples (or rows) together and write it in a compact matrix form like,\n$$ \\hat{y} = h_{\\theta}(X) = X \\theta $$\n$X$ is of the shape $(n,d)$ and $θ$ is of the shape $(d,1)$.\n$h_θ(X)$ and $\\hat{y}$ are of the shape $(n,1)$ which are the prices of $n$ houses stored in a vector.\nLoss Function From the above discussion, you must\u0026rsquo;ve figured out that the goal is to find the right value of $\\theta$. That is, the $\\theta$ which gives the correct value or value as close as possible to $y$, given $X$.\nTherefore, you can say that if you\u0026rsquo;re predicted vector $\\hat{y}$ is close to the actual vector $y$, then you are proceding in the right direction with respect to $\\theta$.\nSo, the goal is to minimize the difference between $y$ and $\\hat{y}$.\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n |\\hat{y}^{(i)} - y^{(i)}| $$\nwhere:\n$\\theta^{*}$ is the optimal(right) value of $\\theta$. $\\hat{y} = X\\theta$ represents the predicted values $y$ is the actual output vector The term $|\\hat{y}^{(i)} - y^{(i)}|$ represents the represents the error (or residual) for each data point. Modulus ensures that all errors are positive, so that underestimations and overestimations don’t cancel each other out. But, in practice, we DO NOT use the above equation. We use,\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$\nThis is because:\nMathematical convenience: The squared function is differentiable everywhere, making it easier to differentiate and compute derivatives compared to the modulus function which is not differentiable at it\u0026rsquo;s minimum (at zero). Square function vs modulus function\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nCalifornia Housing Dataset\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ h_{\\theta}(x^{(i)}) = \\theta_{1} x_1^{(i)} + \\theta_{2} x_2^{(i)} + \\theta_{3} x_3^{(i)} + \\dots + \\theta_{d} x_d^{(i)} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n$x^{(i)}$ is of the shape $(1,d)$ and $θ$ is of the shape $(d,1)$ (you can see that the shapes are compatible and you can do matrix multiplication)\n$h_θ(x^{(i)})$ is of the shape $(1,1)$ which is nothing but a scalar and it is what you expect because it is the price of a single house.\nNow, we can stack all the examples (or rows) together and write it in a compact matrix form like,\n$$ \\hat{y} = h_{\\theta}(X) = X \\theta $$\n$X$ is of the shape $(n,d)$ and $θ$ is of the shape $(d,1)$.\n$h_θ(X)$ and $\\hat{y}$ are of the shape $(n,1)$ which are the prices of $n$ houses stored in a vector.\nLoss Function From the above discussion, you must\u0026rsquo;ve figured out that the goal is to find the right value of $\\theta$. That is, the $\\theta$ which gives the correct value or value as close as possible to $y$, given $X$.\nTherefore, you can say that if you\u0026rsquo;re predicted vector $\\hat{y}$ is close to the actual vector $y$, then you are proceding in the right direction with respect to $\\theta$.\nSo, the goal is to minimize the difference between $y$ and $\\hat{y}$.\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n |\\hat{y}^{(i)} - y^{(i)}| $$\nwhere:\n$\\theta^{*}$ is the optimal(right) value of $\\theta$. $\\hat{y} = X\\theta$ represents the predicted values $y$ is the actual output vector The term $|\\hat{y}^{(i)} - y^{(i)}|$ represents the represents the error (or residual) for each data point. Modulus ensures that all errors are positive, so that underestimations and overestimations don’t cancel each other out. But, in practice, we DO NOT use the above equation. We use,\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$\nThis is because:\nMathematical convenience: The squared function is differentiable everywhere, making it easier to differentiate and compute derivatives compared to the modulus function which is not differentiable at it\u0026rsquo;s minimum (at zero). Square function vs modulus function\n$$ Loss function, J(\\theta) = \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 \\theta^{*} = \\underset{\\theta}{argmin} J(\\theta)\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nCalifornia Housing Dataset\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ h_{\\theta}(x^{(i)}) = \\theta_{1} x_1^{(i)} + \\theta_{2} x_2^{(i)} + \\theta_{3} x_3^{(i)} + \\dots + \\theta_{d} x_d^{(i)} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n$x^{(i)}$ is of the shape $(1,d)$ and $θ$ is of the shape $(d,1)$ (you can see that the shapes are compatible and you can do matrix multiplication)\n$h_θ(x^{(i)})$ is of the shape $(1,1)$ which is nothing but a scalar and it is what you expect because it is the price of a single house.\nNow, we can stack all the examples (or rows) together and write it in a compact matrix form like,\n$$ \\hat{y} = h_{\\theta}(X) = X \\theta $$\n$X$ is of the shape $(n,d)$ and $θ$ is of the shape $(d,1)$.\n$h_θ(X)$ and $\\hat{y}$ are of the shape $(n,1)$ which are the prices of $n$ houses stored in a vector.\nLoss Function From the above discussion, you must\u0026rsquo;ve figured out that the goal is to find the right value of $\\theta$. That is, the $\\theta$ which gives the correct value or value as close as possible to $y$, given $X$.\nTherefore, you can say that if you\u0026rsquo;re predicted vector $\\hat{y}$ is close to the actual vector $y$, then you are proceding in the right direction with respect to $\\theta$.\nSo, the goal is to minimize the difference between $y$ and $\\hat{y}$.\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n |\\hat{y}^{(i)} - y^{(i)}| $$\nwhere:\n$\\theta^{*}$ is the optimal(right) value of $\\theta$. $\\hat{y} = X\\theta$ represents the predicted values $y$ is the actual output vector The term $|\\hat{y}^{(i)} - y^{(i)}|$ represents the represents the error (or residual) for each data point. Modulus ensures that all errors are positive, so that underestimations and overestimations don’t cancel each other out. But, in practice, we DO NOT use the above equation. We use,\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$\nThis is because:\nMathematical convenience: The squared function is differentiable everywhere, making it easier to differentiate and compute derivatives compared to the modulus function which is not differentiable at it\u0026rsquo;s minimum (at zero). Square function vs modulus function\n$$ Loss function, J(\\theta) = \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 \\theta^{*} = \\underset{\\theta}{argmin} J(\\theta) $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nCalifornia Housing Dataset\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ h_{\\theta}(x^{(i)}) = \\theta_{1} x_1^{(i)} + \\theta_{2} x_2^{(i)} + \\theta_{3} x_3^{(i)} + \\dots + \\theta_{d} x_d^{(i)} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n$x^{(i)}$ is of the shape $(1,d)$ and $θ$ is of the shape $(d,1)$ (you can see that the shapes are compatible and you can do matrix multiplication)\n$h_θ(x^{(i)})$ is of the shape $(1,1)$ which is nothing but a scalar and it is what you expect because it is the price of a single house.\nNow, we can stack all the examples (or rows) together and write it in a compact matrix form like,\n$$ \\hat{y} = h_{\\theta}(X) = X \\theta $$\n$X$ is of the shape $(n,d)$ and $θ$ is of the shape $(d,1)$.\n$h_θ(X)$ and $\\hat{y}$ are of the shape $(n,1)$ which are the prices of $n$ houses stored in a vector.\nLoss Function From the above discussion, you must\u0026rsquo;ve figured out that the goal is to find the right value of $\\theta$. That is, the $\\theta$ which gives the correct value or value as close as possible to $y$, given $X$.\nTherefore, you can say that if you\u0026rsquo;re predicted vector $\\hat{y}$ is close to the actual vector $y$, then you are proceding in the right direction with respect to $\\theta$.\nSo, the goal is to minimize the difference between $y$ and $\\hat{y}$.\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n |\\hat{y}^{(i)} - y^{(i)}| $$\nwhere:\n$\\theta^{*}$ is the optimal(right) value of $\\theta$. $\\hat{y} = X\\theta$ represents the predicted values $y$ is the actual output vector The term $|\\hat{y}^{(i)} - y^{(i)}|$ represents the represents the error (or residual) for each data point. Modulus ensures that all errors are positive, so that underestimations and overestimations don’t cancel each other out. But, in practice, we DO NOT use the above equation. We use,\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$\nThis is because:\nMathematical convenience: The squared function is differentiable everywhere, making it easier to differentiate and compute derivatives compared to the modulus function which is not differentiable at it\u0026rsquo;s minimum (at zero). Square function vs modulus function\n$$ Loss function, J(\\theta) = \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$\n$$ \\theta^{*} = \\underset{\\theta}{argmin} J(\\theta) $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nCalifornia Housing Dataset\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ h_{\\theta}(x^{(i)}) = \\theta_{1} x_1^{(i)} + \\theta_{2} x_2^{(i)} + \\theta_{3} x_3^{(i)} + \\dots + \\theta_{d} x_d^{(i)} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n$x^{(i)}$ is of the shape $(1,d)$ and $θ$ is of the shape $(d,1)$ (you can see that the shapes are compatible and you can do matrix multiplication)\n$h_θ(x^{(i)})$ is of the shape $(1,1)$ which is nothing but a scalar and it is what you expect because it is the price of a single house.\nNow, we can stack all the examples (or rows) together and write it in a compact matrix form like,\n$$ \\hat{y} = h_{\\theta}(X) = X \\theta $$\n$X$ is of the shape $(n,d)$ and $θ$ is of the shape $(d,1)$.\n$h_θ(X)$ and $\\hat{y}$ are of the shape $(n,1)$ which are the prices of $n$ houses stored in a vector.\nLoss Function From the above discussion, you must\u0026rsquo;ve figured out that the goal is to find the right value of $\\theta$. That is, the $\\theta$ which gives the correct value or value as close as possible to $y$, given $X$.\nTherefore, you can say that if you\u0026rsquo;re predicted vector $\\hat{y}$ is close to the actual vector $y$, then you are proceding in the right direction with respect to $\\theta$.\nSo, the goal is to minimize the difference between $y$ and $\\hat{y}$.\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n |\\hat{y}^{(i)} - y^{(i)}| $$\nwhere:\n$\\theta^{*}$ is the optimal(right) value of $\\theta$. $\\hat{y} = X\\theta$ represents the predicted values $y$ is the actual output vector The term $|\\hat{y}^{(i)} - y^{(i)}|$ represents the represents the error (or residual) for each data point. Modulus ensures that all errors are positive, so that underestimations and overestimations don’t cancel each other out. But, in practice, we DO NOT use the above equation. We use,\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$\nThis is because:\nMathematical convenience: The squared function is differentiable everywhere, making it easier to differentiate and compute derivatives compared to the modulus function which is not differentiable at it\u0026rsquo;s minimum (at zero). Square function vs modulus function\n$$ Loss function, J(\\theta) = \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$\n$$ \\theta^{*} = \\underset{\\theta}{argmin} J(\\theta) $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nCalifornia Housing Dataset\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ h_{\\theta}(x^{(i)}) = \\theta_{1} x_1^{(i)} + \\theta_{2} x_2^{(i)} + \\theta_{3} x_3^{(i)} + \\dots + \\theta_{d} x_d^{(i)} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n$x^{(i)}$ is of the shape $(1,d)$ and $θ$ is of the shape $(d,1)$ (you can see that the shapes are compatible and you can do matrix multiplication)\n$h_θ(x^{(i)})$ is of the shape $(1,1)$ which is nothing but a scalar and it is what you expect because it is the price of a single house.\nNow, we can stack all the examples (or rows) together and write it in a compact matrix form like,\n$$ \\hat{y} = h_{\\theta}(X) = X \\theta $$\n$X$ is of the shape $(n,d)$ and $θ$ is of the shape $(d,1)$.\n$h_θ(X)$ and $\\hat{y}$ are of the shape $(n,1)$ which are the prices of $n$ houses stored in a vector.\nLoss Function From the above discussion, you must\u0026rsquo;ve figured out that the goal is to find the right value of $\\theta$. That is, the $\\theta$ which gives the correct value or value as close as possible to $y$, given $X$.\nTherefore, you can say that if you\u0026rsquo;re predicted vector $\\hat{y}$ is close to the actual vector $y$, then you are proceding in the right direction with respect to $\\theta$.\nSo, the goal is to minimize the difference between $y$ and $\\hat{y}$.\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n |\\hat{y}^{(i)} - y^{(i)}| $$\nwhere:\n$\\theta^{*}$ is the optimal(right) value of $\\theta$. $\\hat{y} = X\\theta$ represents the predicted values $y$ is the actual output vector The term $|\\hat{y}^{(i)} - y^{(i)}|$ represents the represents the error (or residual) for each data point. Modulus ensures that all errors are positive, so that underestimations and overestimations don’t cancel each other out. But, in practice, we DO NOT use the above equation. We use,\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$\nThis is because:\nMathematical convenience: The squared function is differentiable everywhere, making it easier to differentiate and compute derivatives compared to the modulus function which is not differentiable at it\u0026rsquo;s minimum (at zero). Square function vs modulus function\n$$ Loss function, J(\\theta) = \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$\n$$ \\theta^{*} = \\underset{\\theta}{argmin} J(\\theta) $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nCalifornia Housing Dataset\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ h_{\\theta}(x^{(i)}) = \\theta_{1} x_1^{(i)} + \\theta_{2} x_2^{(i)} + \\theta_{3} x_3^{(i)} + \\dots + \\theta_{d} x_d^{(i)} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n$x^{(i)}$ is of the shape $(1,d)$ and $θ$ is of the shape $(d,1)$ (you can see that the shapes are compatible and you can do matrix multiplication)\n$h_θ(x^{(i)})$ is of the shape $(1,1)$ which is nothing but a scalar and it is what you expect because it is the price of a single house.\nNow, we can stack all the examples (or rows) together and write it in a compact matrix form like,\n$$ \\hat{y} = h_{\\theta}(X) = X \\theta $$\n$X$ is of the shape $(n,d)$ and $θ$ is of the shape $(d,1)$.\n$h_θ(X)$ and $\\hat{y}$ are of the shape $(n,1)$ which are the prices of $n$ houses stored in a vector.\nLoss Function From the above discussion, you must\u0026rsquo;ve figured out that the goal is to find the right value of $\\theta$. That is, the $\\theta$ which gives the correct value or value as close as possible to $y$, given $X$.\nTherefore, you can say that if you\u0026rsquo;re predicted vector $\\hat{y}$ is close to the actual vector $y$, then you are proceding in the right direction with respect to $\\theta$.\nSo, the goal is to minimize the difference between $y$ and $\\hat{y}$.\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n |\\hat{y}^{(i)} - y^{(i)}| $$\nwhere:\n$\\theta^{*}$ is the optimal(right) value of $\\theta$. $\\hat{y} = X\\theta$ represents the predicted values $y$ is the actual output vector The term $|\\hat{y}^{(i)} - y^{(i)}|$ represents the represents the error (or residual) for each data point. Modulus ensures that all errors are positive, so that underestimations and overestimations don’t cancel each other out. But, in practice, we DO NOT use the above equation. We use,\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$\nThis is because:\nMathematical convenience: The squared function is differentiable everywhere, making it easier to differentiate and compute derivatives compared to the modulus function which is not differentiable at it\u0026rsquo;s minimum (at zero). Square function vs modulus function\n$$ Loss function, J(\\theta) = \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$\n$$ \\theta^{*} = \\underset{\\theta}{argmin} J(\\theta) $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nCalifornia Housing Dataset\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ h_{\\theta}(x^{(i)}) = \\theta_{1} x_1^{(i)} + \\theta_{2} x_2^{(i)} + \\theta_{3} x_3^{(i)} + \\dots + \\theta_{d} x_d^{(i)} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n$x^{(i)}$ is of the shape $(1,d)$ and $θ$ is of the shape $(d,1)$ (you can see that the shapes are compatible and you can do matrix multiplication)\n$h_θ(x^{(i)})$ is of the shape $(1,1)$ which is nothing but a scalar and it is what you expect because it is the price of a single house.\nNow, we can stack all the examples (or rows) together and write it in a compact matrix form like,\n$$ \\hat{y} = h_{\\theta}(X) = X \\theta $$\n$X$ is of the shape $(n,d)$ and $θ$ is of the shape $(d,1)$.\n$h_θ(X)$ and $\\hat{y}$ are of the shape $(n,1)$ which are the prices of $n$ houses stored in a vector.\nLoss Function From the above discussion, you must\u0026rsquo;ve figured out that the goal is to find the right value of $\\theta$. That is, the $\\theta$ which gives the correct value or value as close as possible to $y$, given $X$.\nTherefore, you can say that if you\u0026rsquo;re predicted vector $\\hat{y}$ is close to the actual vector $y$, then you are proceding in the right direction with respect to $\\theta$.\nSo, the goal is to minimize the difference between $y$ and $\\hat{y}$.\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n |\\hat{y}^{(i)} - y^{(i)}| $$\nwhere:\n$\\theta^{*}$ is the optimal(right) value of $\\theta$. $\\hat{y} = X\\theta$ represents the predicted values $y$ is the actual output vector The term $|\\hat{y}^{(i)} - y^{(i)}|$ represents the represents the error (or residual) for each data point. Modulus ensures that all errors are positive, so that underestimations and overestimations don’t cancel each other out. But, in practice, we DO NOT use the above equation. We use,\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$\nThis is because:\nMathematical convenience: The squared function is differentiable everywhere, making it easier to differentiate and compute derivatives compared to the modulus function which is not differentiable at it\u0026rsquo;s minimum (at zero). Square function vs modulus function\nLoss function $J(\\theta) = \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2$\n$$ \\theta^{*} = \\underset{\\theta}{argmin} J(\\theta) $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nCalifornia Housing Dataset\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ h_{\\theta}(x^{(i)}) = \\theta_{1} x_1^{(i)} + \\theta_{2} x_2^{(i)} + \\theta_{3} x_3^{(i)} + \\dots + \\theta_{d} x_d^{(i)} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n$x^{(i)}$ is of the shape $(1,d)$ and $θ$ is of the shape $(d,1)$ (you can see that the shapes are compatible and you can do matrix multiplication)\n$h_θ(x^{(i)})$ is of the shape $(1,1)$ which is nothing but a scalar and it is what you expect because it is the price of a single house.\nNow, we can stack all the examples (or rows) together and write it in a compact matrix form like,\n$$ \\hat{y} = h_{\\theta}(X) = X \\theta $$\n$X$ is of the shape $(n,d)$ and $θ$ is of the shape $(d,1)$.\n$h_θ(X)$ and $\\hat{y}$ are of the shape $(n,1)$ which are the prices of $n$ houses stored in a vector.\nLoss Function From the above discussion, you must\u0026rsquo;ve figured out that the goal is to find the right value of $\\theta$. That is, the $\\theta$ which gives the correct value or value as close as possible to $y$, given $X$.\nTherefore, you can say that if you\u0026rsquo;re predicted vector $\\hat{y}$ is close to the actual vector $y$, then you are proceding in the right direction with respect to $\\theta$.\nSo, the goal is to minimize the difference between $y$ and $\\hat{y}$.\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n |\\hat{y}^{(i)} - y^{(i)}| $$\nwhere:\n$\\theta^{*}$ is the optimal(right) value of $\\theta$. $\\hat{y} = X\\theta$ represents the predicted values $y$ is the actual output vector The term $|\\hat{y}^{(i)} - y^{(i)}|$ represents the represents the error (or residual) for each data point. Modulus ensures that all errors are positive, so that underestimations and overestimations don’t cancel each other out. But, in practice, we DO NOT use the above equation. We use,\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$\nThis is because:\nMathematical convenience: The squared function is differentiable everywhere, making it easier to differentiate and compute derivatives compared to the modulus function which is not differentiable at it\u0026rsquo;s minimum (at zero). Square function vs modulus function\nLoss function, $J(\\theta) = \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2$\n$$ \\theta^{*} = \\underset{\\theta}{argmin} J(\\theta) $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nCalifornia Housing Dataset\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ h_{\\theta}(x^{(i)}) = \\theta_{1} x_1^{(i)} + \\theta_{2} x_2^{(i)} + \\theta_{3} x_3^{(i)} + \\dots + \\theta_{d} x_d^{(i)} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n$x^{(i)}$ is of the shape $(1,d)$ and $θ$ is of the shape $(d,1)$ (you can see that the shapes are compatible and you can do matrix multiplication)\n$h_θ(x^{(i)})$ is of the shape $(1,1)$ which is nothing but a scalar and it is what you expect because it is the price of a single house.\nNow, we can stack all the examples (or rows) together and write it in a compact matrix form like,\n$$ \\hat{y} = h_{\\theta}(X) = X \\theta $$\n$X$ is of the shape $(n,d)$ and $θ$ is of the shape $(d,1)$.\n$h_θ(X)$ and $\\hat{y}$ are of the shape $(n,1)$ which are the prices of $n$ houses stored in a vector.\nLoss Function From the above discussion, you must\u0026rsquo;ve figured out that the goal is to find the right value of $\\theta$. That is, the $\\theta$ which gives the correct value or value as close as possible to $y$, given $X$.\nTherefore, you can say that if you\u0026rsquo;re predicted vector $\\hat{y}$ is close to the actual vector $y$, then you are proceding in the right direction with respect to $\\theta$.\nSo, the goal is to minimize the difference between $y$ and $\\hat{y}$.\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n |\\hat{y}^{(i)} - y^{(i)}| $$\nwhere:\n$\\theta^{*}$ is the optimal(right) value of $\\theta$. $\\hat{y} = X\\theta$ represents the predicted values $y$ is the actual output vector The term $|\\hat{y}^{(i)} - y^{(i)}|$ represents the represents the error (or residual) for each data point. Modulus ensures that all errors are positive, so that underestimations and overestimations don’t cancel each other out. But, in practice, we DO NOT use the above equation. We use,\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$\nThis is because:\nMathematical convenience: The squared function is differentiable everywhere, making it easier to differentiate and compute derivatives compared to the modulus function which is not differentiable at it\u0026rsquo;s minimum (at zero). Square function vs modulus function\nLoss function, $J(\\theta) = \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 = |\\hat{y} - y|_2$\n$$ \\theta^{*} = \\underset{\\theta}{argmin} J(\\theta) $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nCalifornia Housing Dataset\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ h_{\\theta}(x^{(i)}) = \\theta_{1} x_1^{(i)} + \\theta_{2} x_2^{(i)} + \\theta_{3} x_3^{(i)} + \\dots + \\theta_{d} x_d^{(i)} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n$x^{(i)}$ is of the shape $(1,d)$ and $θ$ is of the shape $(d,1)$ (you can see that the shapes are compatible and you can do matrix multiplication)\n$h_θ(x^{(i)})$ is of the shape $(1,1)$ which is nothing but a scalar and it is what you expect because it is the price of a single house.\nNow, we can stack all the examples (or rows) together and write it in a compact matrix form like,\n$$ \\hat{y} = h_{\\theta}(X) = X \\theta $$\n$X$ is of the shape $(n,d)$ and $θ$ is of the shape $(d,1)$.\n$h_θ(X)$ and $\\hat{y}$ are of the shape $(n,1)$ which are the prices of $n$ houses stored in a vector.\nLoss Function From the above discussion, you must\u0026rsquo;ve figured out that the goal is to find the right value of $\\theta$. That is, the $\\theta$ which gives the correct value or value as close as possible to $y$, given $X$.\nTherefore, you can say that if you\u0026rsquo;re predicted vector $\\hat{y}$ is close to the actual vector $y$, then you are proceding in the right direction with respect to $\\theta$.\nSo, the goal is to minimize the difference between $y$ and $\\hat{y}$.\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n |\\hat{y}^{(i)} - y^{(i)}| $$\nwhere:\n$\\theta^{*}$ is the optimal(right) value of $\\theta$. $\\hat{y} = X\\theta$ represents the predicted values $y$ is the actual output vector The term $|\\hat{y}^{(i)} - y^{(i)}|$ represents the represents the error (or residual) for each data point. Modulus ensures that all errors are positive, so that underestimations and overestimations don’t cancel each other out. But, in practice, we DO NOT use the above equation. We use,\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$\nThis is because:\nMathematical convenience: The squared function is differentiable everywhere, making it easier to differentiate and compute derivatives compared to the modulus function which is not differentiable at it\u0026rsquo;s minimum (at zero). Square function vs modulus function\nLoss function, $J(\\theta) = \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 = ||\\hat{y} - y||_2$\n$$ \\theta^{*} = \\underset{\\theta}{argmin} J(\\theta) $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nCalifornia Housing Dataset\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ h_{\\theta}(x^{(i)}) = \\theta_{1} x_1^{(i)} + \\theta_{2} x_2^{(i)} + \\theta_{3} x_3^{(i)} + \\dots + \\theta_{d} x_d^{(i)} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n$x^{(i)}$ is of the shape $(1,d)$ and $θ$ is of the shape $(d,1)$ (you can see that the shapes are compatible and you can do matrix multiplication)\n$h_θ(x^{(i)})$ is of the shape $(1,1)$ which is nothing but a scalar and it is what you expect because it is the price of a single house.\nNow, we can stack all the examples (or rows) together and write it in a compact matrix form like,\n$$ \\hat{y} = h_{\\theta}(X) = X \\theta $$\n$X$ is of the shape $(n,d)$ and $θ$ is of the shape $(d,1)$.\n$h_θ(X)$ and $\\hat{y}$ are of the shape $(n,1)$ which are the prices of $n$ houses stored in a vector.\nLoss Function From the above discussion, you must\u0026rsquo;ve figured out that the goal is to find the right value of $\\theta$. That is, the $\\theta$ which gives the correct value or value as close as possible to $y$, given $X$.\nTherefore, you can say that if you\u0026rsquo;re predicted vector $\\hat{y}$ is close to the actual vector $y$, then you are proceding in the right direction with respect to $\\theta$.\nSo, the goal is to minimize the difference between $y$ and $\\hat{y}$.\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n |\\hat{y}^{(i)} - y^{(i)}| $$\nwhere:\n$\\theta^{*}$ is the optimal(right) value of $\\theta$. $\\hat{y} = X\\theta$ represents the predicted values $y$ is the actual output vector The term $|\\hat{y}^{(i)} - y^{(i)}|$ represents the represents the error (or residual) for each data point. Modulus ensures that all errors are positive, so that underestimations and overestimations don’t cancel each other out. But, in practice, we DO NOT use the above equation. We use,\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$\nThis is because:\nMathematical convenience: The squared function is differentiable everywhere, making it easier to differentiate and compute derivatives compared to the modulus function which is not differentiable at it\u0026rsquo;s minimum (at zero). Square function vs modulus function\nLet\u0026rsquo;s represent Loss function with $J(\\theta)$\n$$ J(\\theta) = \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 J(\\theta) = ||\\hat{y} - y||_2 \\theta^{*} = \\underset{\\theta}{argmin} J(\\theta) $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nCalifornia Housing Dataset\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ h_{\\theta}(x^{(i)}) = \\theta_{1} x_1^{(i)} + \\theta_{2} x_2^{(i)} + \\theta_{3} x_3^{(i)} + \\dots + \\theta_{d} x_d^{(i)} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n$x^{(i)}$ is of the shape $(1,d)$ and $θ$ is of the shape $(d,1)$ (you can see that the shapes are compatible and you can do matrix multiplication)\n$h_θ(x^{(i)})$ is of the shape $(1,1)$ which is nothing but a scalar and it is what you expect because it is the price of a single house.\nNow, we can stack all the examples (or rows) together and write it in a compact matrix form like,\n$$ \\hat{y} = h_{\\theta}(X) = X \\theta $$\n$X$ is of the shape $(n,d)$ and $θ$ is of the shape $(d,1)$.\n$h_θ(X)$ and $\\hat{y}$ are of the shape $(n,1)$ which are the prices of $n$ houses stored in a vector.\nLoss Function From the above discussion, you must\u0026rsquo;ve figured out that the goal is to find the right value of $\\theta$. That is, the $\\theta$ which gives the correct value or value as close as possible to $y$, given $X$.\nTherefore, you can say that if you\u0026rsquo;re predicted vector $\\hat{y}$ is close to the actual vector $y$, then you are proceding in the right direction with respect to $\\theta$.\nSo, the goal is to minimize the difference between $y$ and $\\hat{y}$.\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n |\\hat{y}^{(i)} - y^{(i)}| $$\nwhere:\n$\\theta^{*}$ is the optimal(right) value of $\\theta$. $\\hat{y} = X\\theta$ represents the predicted values $y$ is the actual output vector The term $|\\hat{y}^{(i)} - y^{(i)}|$ represents the represents the error (or residual) for each data point. Modulus ensures that all errors are positive, so that underestimations and overestimations don’t cancel each other out. But, in practice, we DO NOT use the above equation. We use,\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$\nThis is because:\nMathematical convenience: The squared function is differentiable everywhere, making it easier to differentiate and compute derivatives compared to the modulus function which is not differentiable at it\u0026rsquo;s minimum (at zero). Square function vs modulus function\nLet\u0026rsquo;s represent Loss function with $J(\\theta)$\n$$ J(\\theta) = \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$ $$ J(\\theta) = ||\\hat{y} - y||_2 $$ $$ \\theta^{*} = \\underset{\\theta}{argmin} J(\\theta) $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nCalifornia Housing Dataset\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ h_{\\theta}(x^{(i)}) = \\theta_{1} x_1^{(i)} + \\theta_{2} x_2^{(i)} + \\theta_{3} x_3^{(i)} + \\dots + \\theta_{d} x_d^{(i)} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n$x^{(i)}$ is of the shape $(1,d)$ and $θ$ is of the shape $(d,1)$ (you can see that the shapes are compatible and you can do matrix multiplication)\n$h_θ(x^{(i)})$ is of the shape $(1,1)$ which is nothing but a scalar and it is what you expect because it is the price of a single house.\nNow, we can stack all the examples (or rows) together and write it in a compact matrix form like,\n$$ \\hat{y} = h_{\\theta}(X) = X \\theta $$\n$X$ is of the shape $(n,d)$ and $θ$ is of the shape $(d,1)$.\n$h_θ(X)$ and $\\hat{y}$ are of the shape $(n,1)$ which are the prices of $n$ houses stored in a vector.\nLoss Function From the above discussion, you must\u0026rsquo;ve figured out that the goal is to find the right value of $\\theta$. That is, the $\\theta$ which gives the correct value or value as close as possible to $y$, given $X$.\nTherefore, you can say that if you\u0026rsquo;re predicted vector $\\hat{y}$ is close to the actual vector $y$, then you are proceding in the right direction with respect to $\\theta$.\nSo, the goal is to minimize the difference between $y$ and $\\hat{y}$.\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n |\\hat{y}^{(i)} - y^{(i)}| $$\nwhere:\n$\\theta^{*}$ is the optimal(right) value of $\\theta$. $\\hat{y} = X\\theta$ represents the predicted values $y$ is the actual output vector The term $|\\hat{y}^{(i)} - y^{(i)}|$ represents the represents the error (or residual) for each data point. Modulus ensures that all errors are positive, so that underestimations and overestimations don’t cancel each other out. But, in practice, we DO NOT use the above equation. We use,\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$\nThis is because:\nMathematical convenience: The squared function is differentiable everywhere, making it easier to differentiate and compute derivatives compared to the modulus function which is not differentiable at it\u0026rsquo;s minimum (at zero). Square function vs modulus function\nLet\u0026rsquo;s represent Loss function with $J(\\theta)$. That implies,\n$$ J(\\theta) = \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$ $$ J(\\theta) = ||\\hat{y} - y||_2 $$ $$ \\theta^{*} = \\underset{\\theta}{argmin} J(\\theta) $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nCalifornia Housing Dataset\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ h_{\\theta}(x^{(i)}) = \\theta_{1} x_1^{(i)} + \\theta_{2} x_2^{(i)} + \\theta_{3} x_3^{(i)} + \\dots + \\theta_{d} x_d^{(i)} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n$x^{(i)}$ is of the shape $(1,d)$ and $θ$ is of the shape $(d,1)$ (you can see that the shapes are compatible and you can do matrix multiplication)\n$h_θ(x^{(i)})$ is of the shape $(1,1)$ which is nothing but a scalar and it is what you expect because it is the price of a single house.\nNow, we can stack all the examples (or rows) together and write it in a compact matrix form like,\n$$ \\hat{y} = h_{\\theta}(X) = X \\theta $$\n$X$ is of the shape $(n,d)$ and $θ$ is of the shape $(d,1)$.\n$h_θ(X)$ and $\\hat{y}$ are of the shape $(n,1)$ which are the prices of $n$ houses stored in a vector.\nLoss Function From the above discussion, you must\u0026rsquo;ve figured out that the goal is to find the right value of $\\theta$. That is, the $\\theta$ which gives the correct value or value as close as possible to $y$, given $X$.\nTherefore, you can say that if you\u0026rsquo;re predicted vector $\\hat{y}$ is close to the actual vector $y$, then you are proceding in the right direction with respect to $\\theta$.\nSo, the goal is to minimize the difference between $y$ and $\\hat{y}$.\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n |\\hat{y}^{(i)} - y^{(i)}| $$\nwhere:\n$\\theta^{*}$ is the optimal(right) value of $\\theta$. $\\hat{y} = X\\theta$ represents the predicted values $y$ is the actual output vector The term $|\\hat{y}^{(i)} - y^{(i)}|$ represents the represents the error (or residual) for each data point. Modulus ensures that all errors are positive, so that underestimations and overestimations don’t cancel each other out. But, in practice, we DO NOT use the above equation. We use,\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$\nThis is because:\nMathematical convenience: The squared function is differentiable everywhere, making it easier to differentiate and compute derivatives compared to the modulus function which is not differentiable at it\u0026rsquo;s minimum (at zero). Square function vs modulus function\nLet\u0026rsquo;s represent Loss function with $J(\\theta)$. That implies,\n$$ J(\\theta) = \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$ $$ J(\\theta) = ||\\hat{y} - y||_2 (Vector representation. L2 norm.) $$ $$ \\theta^{*} = \\underset{\\theta}{argmin} J(\\theta) $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nCalifornia Housing Dataset\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ h_{\\theta}(x^{(i)}) = \\theta_{1} x_1^{(i)} + \\theta_{2} x_2^{(i)} + \\theta_{3} x_3^{(i)} + \\dots + \\theta_{d} x_d^{(i)} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n$x^{(i)}$ is of the shape $(1,d)$ and $θ$ is of the shape $(d,1)$ (you can see that the shapes are compatible and you can do matrix multiplication)\n$h_θ(x^{(i)})$ is of the shape $(1,1)$ which is nothing but a scalar and it is what you expect because it is the price of a single house.\nNow, we can stack all the examples (or rows) together and write it in a compact matrix form like,\n$$ \\hat{y} = h_{\\theta}(X) = X \\theta $$\n$X$ is of the shape $(n,d)$ and $θ$ is of the shape $(d,1)$.\n$h_θ(X)$ and $\\hat{y}$ are of the shape $(n,1)$ which are the prices of $n$ houses stored in a vector.\nLoss Function From the above discussion, you must\u0026rsquo;ve figured out that the goal is to find the right value of $\\theta$. That is, the $\\theta$ which gives the correct value or value as close as possible to $y$, given $X$.\nTherefore, you can say that if you\u0026rsquo;re predicted vector $\\hat{y}$ is close to the actual vector $y$, then you are proceding in the right direction with respect to $\\theta$.\nSo, the goal is to minimize the difference between $y$ and $\\hat{y}$.\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n |\\hat{y}^{(i)} - y^{(i)}| $$\nwhere:\n$\\theta^{*}$ is the optimal(right) value of $\\theta$. $\\hat{y} = X\\theta$ represents the predicted values $y$ is the actual output vector The term $|\\hat{y}^{(i)} - y^{(i)}|$ represents the represents the error (or residual) for each data point. Modulus ensures that all errors are positive, so that underestimations and overestimations don’t cancel each other out. But, in practice, we DO NOT use the above equation. We use,\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$\nThis is because:\nMathematical convenience: The squared function is differentiable everywhere, making it easier to differentiate and compute derivatives compared to the modulus function which is not differentiable at it\u0026rsquo;s minimum (at zero). Square function vs modulus function\nLet\u0026rsquo;s represent Loss function with $J(\\theta)$. That implies,\n$$ J(\\theta) = \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$ $$ J(\\theta) = ||\\hat{y} - y||_2 $$ $$ \\theta^{*} = \\underset{\\theta}{argmin} J(\\theta) $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nCalifornia Housing Dataset\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ h_{\\theta}(x^{(i)}) = \\theta_{1} x_1^{(i)} + \\theta_{2} x_2^{(i)} + \\theta_{3} x_3^{(i)} + \\dots + \\theta_{d} x_d^{(i)} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n$x^{(i)}$ is of the shape $(1,d)$ and $θ$ is of the shape $(d,1)$ (you can see that the shapes are compatible and you can do matrix multiplication)\n$h_θ(x^{(i)})$ is of the shape $(1,1)$ which is nothing but a scalar and it is what you expect because it is the price of a single house.\nNow, we can stack all the examples (or rows) together and write it in a compact matrix form like,\n$$ \\hat{y} = h_{\\theta}(X) = X \\theta $$\n$X$ is of the shape $(n,d)$ and $θ$ is of the shape $(d,1)$.\n$h_θ(X)$ and $\\hat{y}$ are of the shape $(n,1)$ which are the prices of $n$ houses stored in a vector.\nLoss Function From the above discussion, you must\u0026rsquo;ve figured out that the goal is to find the right value of $\\theta$. That is, the $\\theta$ which gives the correct value or value as close as possible to $y$, given $X$.\nTherefore, you can say that if you\u0026rsquo;re predicted vector $\\hat{y}$ is close to the actual vector $y$, then you are proceding in the right direction with respect to $\\theta$.\nSo, the goal is to minimize the difference between $y$ and $\\hat{y}$.\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n |\\hat{y}^{(i)} - y^{(i)}| $$\nwhere:\n$\\theta^{*}$ is the optimal(right) value of $\\theta$. $\\hat{y} = X\\theta$ represents the predicted values $y$ is the actual output vector The term $|\\hat{y}^{(i)} - y^{(i)}|$ represents the represents the error (or residual) for each data point. Modulus ensures that all errors are positive, so that underestimations and overestimations don’t cancel each other out. But, in practice, we DO NOT use the above equation. We use,\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$\nThis is because:\nMathematical convenience: The squared function is differentiable everywhere, making it easier to differentiate and compute derivatives compared to the modulus function which is not differentiable at it\u0026rsquo;s minimum (at zero). Square function vs modulus function\nLet\u0026rsquo;s represent Loss function with $J(\\theta)$. That implies,\n$$ J(\\theta) = \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$ $$ J(\\theta) = ||\\hat{y} - y||_2 $$ $$ \\theta^{*} = \\underset{\\theta}{\\mathrm{argmin}} J(\\theta) $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nCalifornia Housing Dataset\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ h_{\\theta}(x^{(i)}) = \\theta_{1} x_1^{(i)} + \\theta_{2} x_2^{(i)} + \\theta_{3} x_3^{(i)} + \\dots + \\theta_{d} x_d^{(i)} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n$x^{(i)}$ is of the shape $(1,d)$ and $θ$ is of the shape $(d,1)$ (you can see that the shapes are compatible and you can do matrix multiplication)\n$h_θ(x^{(i)})$ is of the shape $(1,1)$ which is nothing but a scalar and it is what you expect because it is the price of a single house.\nNow, we can stack all the examples (or rows) together and write it in a compact matrix form like,\n$$ \\hat{y} = h_{\\theta}(X) = X \\theta $$\n$X$ is of the shape $(n,d)$ and $θ$ is of the shape $(d,1)$.\n$h_θ(X)$ and $\\hat{y}$ are of the shape $(n,1)$ which are the prices of $n$ houses stored in a vector.\nLoss Function From the above discussion, you must\u0026rsquo;ve figured out that the goal is to find the right value of $\\theta$. That is, the $\\theta$ which gives the correct value or value as close as possible to $y$, given $X$.\nTherefore, you can say that if you\u0026rsquo;re predicted vector $\\hat{y}$ is close to the actual vector $y$, then you are proceding in the right direction with respect to $\\theta$.\nSo, the goal is to minimize the difference between $y$ and $\\hat{y}$.\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n |\\hat{y}^{(i)} - y^{(i)}| $$\nwhere:\n$\\theta^{*}$ is the optimal(right) value of $\\theta$. $\\hat{y} = X\\theta$ represents the predicted values $y$ is the actual output vector The term $|\\hat{y}^{(i)} - y^{(i)}|$ represents the represents the error (or residual) for each data point. Modulus ensures that all errors are positive, so that underestimations and overestimations don’t cancel each other out. But, in practice, we DO NOT use the above equation. We use,\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$\nThis is because:\nMathematical convenience: The squared function is differentiable everywhere, making it easier to differentiate and compute derivatives compared to the modulus function which is not differentiable at it\u0026rsquo;s minimum (at zero). Square function vs modulus function\nLet\u0026rsquo;s represent Loss function with $J(\\theta)$. That implies,\n$$ J(\\theta) = \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$ $$ J(\\theta) = ||\\hat{y} - y||_2 $$ $$ \\theta^{*} = \\underset{\\theta}{argmin} J(\\theta) $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nCalifornia Housing Dataset\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ h_{\\theta}(x^{(i)}) = \\theta_{1} x_1^{(i)} + \\theta_{2} x_2^{(i)} + \\theta_{3} x_3^{(i)} + \\dots + \\theta_{d} x_d^{(i)} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n$x^{(i)}$ is of the shape $(1,d)$ and $θ$ is of the shape $(d,1)$ (you can see that the shapes are compatible and you can do matrix multiplication)\n$h_θ(x^{(i)})$ is of the shape $(1,1)$ which is nothing but a scalar and it is what you expect because it is the price of a single house.\nNow, we can stack all the examples (or rows) together and write it in a compact matrix form like,\n$$ \\hat{y} = h_{\\theta}(X) = X \\theta $$\n$X$ is of the shape $(n,d)$ and $θ$ is of the shape $(d,1)$.\n$h_θ(X)$ and $\\hat{y}$ are of the shape $(n,1)$ which are the prices of $n$ houses stored in a vector.\nLoss Function From the above discussion, you must\u0026rsquo;ve figured out that the goal is to find the right value of $\\theta$. That is, the $\\theta$ which gives the correct value or value as close as possible to $y$, given $X$.\nTherefore, you can say that if you\u0026rsquo;re predicted vector $\\hat{y}$ is close to the actual vector $y$, then you are proceding in the right direction with respect to $\\theta$.\nSo, the goal is to minimize the difference between $y$ and $\\hat{y}$.\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n |\\hat{y}^{(i)} - y^{(i)}| $$\nwhere:\n$\\theta^{*}$ is the optimal(right) value of $\\theta$. $\\hat{y} = X\\theta$ represents the predicted values $y$ is the actual output vector The term $|\\hat{y}^{(i)} - y^{(i)}|$ represents the represents the error (or residual) for each data point. Modulus ensures that all errors are positive, so that underestimations and overestimations don’t cancel each other out. But, in practice, we DO NOT use the above equation. We use,\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$\nThis is because:\nMathematical convenience: The squared function is differentiable everywhere, making it easier to differentiate and compute derivatives compared to the modulus function which is not differentiable at it\u0026rsquo;s minimum (at zero). Square function vs modulus function\nLet\u0026rsquo;s represent Loss function with $J(\\theta)$. That implies,\n$$ J(\\theta) = \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$ $$ J(\\theta) = ||\\hat{y} - y||_2 \\textrm{(Vector representation. L2 norm.)} $$ $$ \\theta^{*} = \\underset{\\theta}{argmin} J(\\theta) $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nCalifornia Housing Dataset\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ h_{\\theta}(x^{(i)}) = \\theta_{1} x_1^{(i)} + \\theta_{2} x_2^{(i)} + \\theta_{3} x_3^{(i)} + \\dots + \\theta_{d} x_d^{(i)} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n$x^{(i)}$ is of the shape $(1,d)$ and $θ$ is of the shape $(d,1)$ (you can see that the shapes are compatible and you can do matrix multiplication)\n$h_θ(x^{(i)})$ is of the shape $(1,1)$ which is nothing but a scalar and it is what you expect because it is the price of a single house.\nNow, we can stack all the examples (or rows) together and write it in a compact matrix form like,\n$$ \\hat{y} = h_{\\theta}(X) = X \\theta $$\n$X$ is of the shape $(n,d)$ and $θ$ is of the shape $(d,1)$.\n$h_θ(X)$ and $\\hat{y}$ are of the shape $(n,1)$ which are the prices of $n$ houses stored in a vector.\nLoss Function From the above discussion, you must\u0026rsquo;ve figured out that the goal is to find the right value of $\\theta$. That is, the $\\theta$ which gives the correct value or value as close as possible to $y$, given $X$.\nTherefore, you can say that if you\u0026rsquo;re predicted vector $\\hat{y}$ is close to the actual vector $y$, then you are proceding in the right direction with respect to $\\theta$.\nSo, the goal is to minimize the difference between $y$ and $\\hat{y}$.\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n |\\hat{y}^{(i)} - y^{(i)}| $$\nwhere:\n$\\theta^{*}$ is the optimal(right) value of $\\theta$. $\\hat{y} = X\\theta$ represents the predicted values $y$ is the actual output vector The term $|\\hat{y}^{(i)} - y^{(i)}|$ represents the represents the error (or residual) for each data point. Modulus ensures that all errors are positive, so that underestimations and overestimations don’t cancel each other out. But, in practice, we DO NOT use the above equation. We use,\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$\nThis is because:\nMathematical convenience: The squared function is differentiable everywhere, making it easier to differentiate and compute derivatives compared to the modulus function which is not differentiable at it\u0026rsquo;s minimum (at zero). Square function vs modulus function\nNow, in Machine Learning, a loss function measures how well our model\u0026rsquo;s predictions match the actual values. It quantifies the error between the predicted output.Let\u0026rsquo;s represent Loss function with $J(\\theta)$. That implies,\n$$ J(\\theta) = \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$ $$ J(\\theta) = ||\\hat{y} - y||_2 $$ $$ \\theta^{*} = \\underset{\\theta}{argmin} J(\\theta) $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nCalifornia Housing Dataset\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ h_{\\theta}(x^{(i)}) = \\theta_{1} x_1^{(i)} + \\theta_{2} x_2^{(i)} + \\theta_{3} x_3^{(i)} + \\dots + \\theta_{d} x_d^{(i)} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n$x^{(i)}$ is of the shape $(1,d)$ and $θ$ is of the shape $(d,1)$ (you can see that the shapes are compatible and you can do matrix multiplication)\n$h_θ(x^{(i)})$ is of the shape $(1,1)$ which is nothing but a scalar and it is what you expect because it is the price of a single house.\nNow, we can stack all the examples (or rows) together and write it in a compact matrix form like,\n$$ \\hat{y} = h_{\\theta}(X) = X \\theta $$\n$X$ is of the shape $(n,d)$ and $θ$ is of the shape $(d,1)$.\n$h_θ(X)$ and $\\hat{y}$ are of the shape $(n,1)$ which are the prices of $n$ houses stored in a vector.\nLoss Function From the above discussion, you must\u0026rsquo;ve figured out that the goal is to find the right value of $\\theta$. That is, the $\\theta$ which gives the correct value or value as close as possible to $y$, given $X$.\nTherefore, you can say that if you\u0026rsquo;re predicted vector $\\hat{y}$ is close to the actual vector $y$, then you are proceding in the right direction with respect to $\\theta$.\nSo, the goal is to minimize the difference between $y$ and $\\hat{y}$.\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n |\\hat{y}^{(i)} - y^{(i)}| $$\nwhere:\n$\\theta^{*}$ is the optimal(right) value of $\\theta$. $\\hat{y} = X\\theta$ represents the predicted values $y$ is the actual output vector The term $|\\hat{y}^{(i)} - y^{(i)}|$ represents the represents the error (or residual) for each data point. Modulus ensures that all errors are positive, so that underestimations and overestimations don’t cancel each other out. But, in practice, we DO NOT use the above equation. We use,\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$\nThis is because:\nMathematical convenience: The squared function is differentiable everywhere, making it easier to differentiate and compute derivatives compared to the modulus function which is not differentiable at it\u0026rsquo;s minimum (at zero). Square function vs modulus function\nNow, in Machine Learning, a loss function measures how well our model\u0026rsquo;s predictions match the actual values. It quantifies the error between the predicted output. Let\u0026rsquo;s represent Loss function with $J(\\theta)$. That implies,\n$$ J(\\theta) = \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$ $$ J(\\theta) = ||\\hat{y} - y||_2 $$ $$ \\theta^{*} = \\underset{\\theta}{argmin} J(\\theta) $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nCalifornia Housing Dataset\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ h_{\\theta}(x^{(i)}) = \\theta_{1} x_1^{(i)} + \\theta_{2} x_2^{(i)} + \\theta_{3} x_3^{(i)} + \\dots + \\theta_{d} x_d^{(i)} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n$x^{(i)}$ is of the shape $(1,d)$ and $θ$ is of the shape $(d,1)$ (you can see that the shapes are compatible and you can do matrix multiplication)\n$h_θ(x^{(i)})$ is of the shape $(1,1)$ which is nothing but a scalar and it is what you expect because it is the price of a single house.\nNow, we can stack all the examples (or rows) together and write it in a compact matrix form like,\n$$ \\hat{y} = h_{\\theta}(X) = X \\theta $$\n$X$ is of the shape $(n,d)$ and $θ$ is of the shape $(d,1)$.\n$h_θ(X)$ and $\\hat{y}$ are of the shape $(n,1)$ which are the prices of $n$ houses stored in a vector.\nLoss Function From the above discussion, you must\u0026rsquo;ve figured out that the goal is to find the right value of $\\theta$. That is, the $\\theta$ which gives the correct value or value as close as possible to $y$, given $X$.\nTherefore, you can say that if you\u0026rsquo;re predicted vector $\\hat{y}$ is close to the actual vector $y$, then you are proceding in the right direction with respect to $\\theta$.\nSo, the goal is to minimize the difference between $y$ and $\\hat{y}$.\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n |\\hat{y}^{(i)} - y^{(i)}| $$\nwhere:\n$\\theta^{*}$ is the optimal(right) value of $\\theta$. $\\hat{y} = X\\theta$ represents the predicted values $y$ is the actual output vector The term $|\\hat{y}^{(i)} - y^{(i)}|$ represents the represents the error (or residual) for each data point. Modulus ensures that all errors are positive, so that underestimations and overestimations don’t cancel each other out. But, in practice, we DO NOT use the above equation. We use,\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$\nThis is because:\nMathematical convenience: The squared function is differentiable everywhere, making it easier to differentiate and compute derivatives compared to the modulus function which is not differentiable at it\u0026rsquo;s minimum (at zero). Square function vs modulus function\nNow, in Machine Learning, a loss function measures how well our model\u0026rsquo;s predictions match the actual values. It quantifies the error between the predicted output $\\hat{y}$ and the actual output $y$. Let\u0026rsquo;s represent Loss function with $J(\\theta)$. That implies,\n$$ J(\\theta) = \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$ $$ J(\\theta) = ||\\hat{y} - y||_2 $$ $$ \\theta^{*} = \\underset{\\theta}{argmin} J(\\theta) $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nCalifornia Housing Dataset\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ h_{\\theta}(x^{(i)}) = \\theta_{1} x_1^{(i)} + \\theta_{2} x_2^{(i)} + \\theta_{3} x_3^{(i)} + \\dots + \\theta_{d} x_d^{(i)} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n$x^{(i)}$ is of the shape $(1,d)$ and $θ$ is of the shape $(d,1)$ (you can see that the shapes are compatible and you can do matrix multiplication)\n$h_θ(x^{(i)})$ is of the shape $(1,1)$ which is nothing but a scalar and it is what you expect because it is the price of a single house.\nNow, we can stack all the examples (or rows) together and write it in a compact matrix form like,\n$$ \\hat{y} = h_{\\theta}(X) = X \\theta $$\n$X$ is of the shape $(n,d)$ and $θ$ is of the shape $(d,1)$.\n$h_θ(X)$ and $\\hat{y}$ are of the shape $(n,1)$ which are the prices of $n$ houses stored in a vector.\nLoss Function From the above discussion, you must\u0026rsquo;ve figured out that the goal is to find the right value of $\\theta$. That is, the $\\theta$ which gives the correct value or value as close as possible to $y$, given $X$.\nTherefore, you can say that if you\u0026rsquo;re predicted vector $\\hat{y}$ is close to the actual vector $y$, then you are proceding in the right direction with respect to $\\theta$.\nSo, the goal is to minimize the difference between $y$ and $\\hat{y}$.\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n |\\hat{y}^{(i)} - y^{(i)}| $$\nwhere:\n$\\theta^{*}$ is the optimal(right) value of $\\theta$. $\\hat{y} = X\\theta$ represents the predicted values $y$ is the actual output vector The term $|\\hat{y}^{(i)} - y^{(i)}|$ represents the represents the error (or residual) for each data point. Modulus ensures that all errors are positive, so that underestimations and overestimations don’t cancel each other out. But, in practice, we DO NOT use the above equation. We use,\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$\nThis is because:\nMathematical convenience: The squared function is differentiable everywhere, making it easier to differentiate and compute derivatives compared to the modulus function which is not differentiable at it\u0026rsquo;s minimum (at zero). Square function vs modulus function\nNow, in Machine Learning, a loss function measures how well our model\u0026rsquo;s predictions match the actual values. It quantifies the error between the predicted output $\\hat{y}$ and the actual output $y$.\nLet\u0026rsquo;s represent Loss function with $J(\\theta)$. That implies,\n$$ J(\\theta) = \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$ $$ J(\\theta) = ||\\hat{y} - y||_2 $$ $$ \\theta^{*} = \\underset{\\theta}{argmin} J(\\theta) $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nCalifornia Housing Dataset\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ h_{\\theta}(x^{(i)}) = \\theta_{1} x_1^{(i)} + \\theta_{2} x_2^{(i)} + \\theta_{3} x_3^{(i)} + \\dots + \\theta_{d} x_d^{(i)} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n$x^{(i)}$ is of the shape $(1,d)$ and $θ$ is of the shape $(d,1)$ (you can see that the shapes are compatible and you can do matrix multiplication)\n$h_θ(x^{(i)})$ is of the shape $(1,1)$ which is nothing but a scalar and it is what you expect because it is the price of a single house.\nNow, we can stack all the examples (or rows) together and write it in a compact matrix form like,\n$$ \\hat{y} = h_{\\theta}(X) = X \\theta $$\n$X$ is of the shape $(n,d)$ and $θ$ is of the shape $(d,1)$.\n$h_θ(X)$ and $\\hat{y}$ are of the shape $(n,1)$ which are the prices of $n$ houses stored in a vector.\nLoss Function From the above discussion, you must\u0026rsquo;ve figured out that the goal is to find the right value of $\\theta$. That is, the $\\theta$ which gives the correct value or value as close as possible to $y$, given $X$.\nTherefore, you can say that if you\u0026rsquo;re predicted vector $\\hat{y}$ is close to the actual vector $y$, then you are proceding in the right direction with respect to $\\theta$.\nSo, the goal is to minimize the difference between $y$ and $\\hat{y}$.\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n |\\hat{y}^{(i)} - y^{(i)}| $$\nwhere:\n$\\theta^{*}$ is the optimal(right) value of $\\theta$. $\\hat{y} = X\\theta$ represents the predicted values $y$ is the actual output vector The term $|\\hat{y}^{(i)} - y^{(i)}|$ represents the represents the error (or residual) for each data point. Modulus ensures that all errors are positive, so that underestimations and overestimations don’t cancel each other out. But, in practice, we DO NOT use the above equation. We use,\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$\nThis is because:\nMathematical convenience: The squared function is differentiable everywhere, making it easier to differentiate and compute derivatives compared to the modulus function which is not differentiable at it\u0026rsquo;s minimum (at zero). Square function vs modulus function\nNow, in Machine Learning, a Loss Function measures how well our model\u0026rsquo;s predictions match the actual values. It quantifies the error between the predicted output $\\hat{y}$ and the actual output $y$.\nLet\u0026rsquo;s represent Loss function with $J(\\theta)$. That implies,\n$$ J(\\theta) = \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$ $$ J(\\theta) = ||\\hat{y} - y||_2 $$ $$ \\theta^{*} = \\underset{\\theta}{argmin} J(\\theta) $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nCalifornia Housing Dataset\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ h_{\\theta}(x^{(i)}) = \\theta_{1} x_1^{(i)} + \\theta_{2} x_2^{(i)} + \\theta_{3} x_3^{(i)} + \\dots + \\theta_{d} x_d^{(i)} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n$x^{(i)}$ is of the shape $(1,d)$ and $θ$ is of the shape $(d,1)$ (you can see that the shapes are compatible and you can do matrix multiplication)\n$h_θ(x^{(i)})$ is of the shape $(1,1)$ which is nothing but a scalar and it is what you expect because it is the price of a single house.\nNow, we can stack all the examples (or rows) together and write it in a compact matrix form like,\n$$ \\hat{y} = h_{\\theta}(X) = X \\theta $$\n$X$ is of the shape $(n,d)$ and $θ$ is of the shape $(d,1)$.\n$h_θ(X)$ and $\\hat{y}$ are of the shape $(n,1)$ which are the prices of $n$ houses stored in a vector.\nLoss Function From the above discussion, you must\u0026rsquo;ve figured out that the goal is to find the right value of $\\theta$. That is, the $\\theta$ which gives the correct value or value as close as possible to $y$, given $X$.\nTherefore, you can say that if you\u0026rsquo;re predicted vector $\\hat{y}$ is close to the actual vector $y$, then you are proceding in the right direction with respect to $\\theta$.\nSo, the goal is to minimize the difference between $y$ and $\\hat{y}$.\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n |\\hat{y}^{(i)} - y^{(i)}| $$\nwhere:\n$\\theta^{*}$ is the optimal(right) value of $\\theta$. $\\hat{y} = X\\theta$ represents the predicted values $y$ is the actual output vector The term $|\\hat{y}^{(i)} - y^{(i)}|$ represents the represents the error (or residual) for each data point. Modulus ensures that all errors are positive, so that underestimations and overestimations don’t cancel each other out. But, in practice, we DO NOT use the above equation. We use,\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$\nThis is because:\nMathematical convenience: The squared function is differentiable everywhere, making it easier to differentiate and compute derivatives compared to the modulus function which is not differentiable at it\u0026rsquo;s minimum (at zero). Square function vs modulus function\nNow, in Machine Learning, a Loss Function measures how well our model\u0026rsquo;s predictions match the actual values. It quantifies the error between the predicted output $\\hat{y}$ and the actual output $y$.\nLet\u0026rsquo;s represent Loss function with $J(\\theta)$. That implies,\n$$ J(\\theta) = \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$ $$ =\u0026gt; J(\\theta) = ||\\hat{y} - y||_2 $$ $$ \\theta^{*} = \\underset{\\theta}{argmin} J(\\theta) $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nCalifornia Housing Dataset\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ h_{\\theta}(x^{(i)}) = \\theta_{1} x_1^{(i)} + \\theta_{2} x_2^{(i)} + \\theta_{3} x_3^{(i)} + \\dots + \\theta_{d} x_d^{(i)} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n$x^{(i)}$ is of the shape $(1,d)$ and $θ$ is of the shape $(d,1)$ (you can see that the shapes are compatible and you can do matrix multiplication)\n$h_θ(x^{(i)})$ is of the shape $(1,1)$ which is nothing but a scalar and it is what you expect because it is the price of a single house.\nNow, we can stack all the examples (or rows) together and write it in a compact matrix form like,\n$$ \\hat{y} = h_{\\theta}(X) = X \\theta $$\n$X$ is of the shape $(n,d)$ and $θ$ is of the shape $(d,1)$.\n$h_θ(X)$ and $\\hat{y}$ are of the shape $(n,1)$ which are the prices of $n$ houses stored in a vector.\nLoss Function From the above discussion, you must\u0026rsquo;ve figured out that the goal is to find the right value of $\\theta$. That is, the $\\theta$ which gives the correct value or value as close as possible to $y$, given $X$.\nTherefore, you can say that if you\u0026rsquo;re predicted vector $\\hat{y}$ is close to the actual vector $y$, then you are proceding in the right direction with respect to $\\theta$.\nSo, the goal is to minimize the difference between $y$ and $\\hat{y}$.\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n |\\hat{y}^{(i)} - y^{(i)}| $$\nwhere:\n$\\theta^{*}$ is the optimal(right) value of $\\theta$. $\\hat{y} = X\\theta$ represents the predicted values $y$ is the actual output vector The term $|\\hat{y}^{(i)} - y^{(i)}|$ represents the represents the error (or residual) for each data point. Modulus ensures that all errors are positive, so that underestimations and overestimations don’t cancel each other out. But, in practice, we DO NOT use the above equation. We use,\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$\nThis is because:\nMathematical convenience: The squared function is differentiable everywhere, making it easier to differentiate and compute derivatives compared to the modulus function which is not differentiable at it\u0026rsquo;s minimum (at zero). Square function vs modulus function\nNow, in Machine Learning, a Loss Function measures how well our model\u0026rsquo;s predictions match the actual values. It quantifies the error between the predicted output $\\hat{y}$ and the actual output $y$.\nLet\u0026rsquo;s represent Loss function with $J(\\theta)$. That implies,\n$$ J(\\theta) = \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$ $$ \\implies J(\\theta) = ||\\hat{y} - y||_2 $$ $$ \\theta^{*} = \\underset{\\theta}{argmin} J(\\theta) $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nCalifornia Housing Dataset\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ h_{\\theta}(x^{(i)}) = \\theta_{1} x_1^{(i)} + \\theta_{2} x_2^{(i)} + \\theta_{3} x_3^{(i)} + \\dots + \\theta_{d} x_d^{(i)} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n$x^{(i)}$ is of the shape $(1,d)$ and $θ$ is of the shape $(d,1)$ (you can see that the shapes are compatible and you can do matrix multiplication)\n$h_θ(x^{(i)})$ is of the shape $(1,1)$ which is nothing but a scalar and it is what you expect because it is the price of a single house.\nNow, we can stack all the examples (or rows) together and write it in a compact matrix form like,\n$$ \\hat{y} = h_{\\theta}(X) = X \\theta $$\n$X$ is of the shape $(n,d)$ and $θ$ is of the shape $(d,1)$.\n$h_θ(X)$ and $\\hat{y}$ are of the shape $(n,1)$ which are the prices of $n$ houses stored in a vector.\nLoss Function From the above discussion, you must\u0026rsquo;ve figured out that the goal is to find the right value of $\\theta$. That is, the $\\theta$ which gives the correct value or value as close as possible to $y$, given $X$.\nTherefore, you can say that if you\u0026rsquo;re predicted vector $\\hat{y}$ is close to the actual vector $y$, then you are proceding in the right direction with respect to $\\theta$.\nSo, the goal is to minimize the difference between $y$ and $\\hat{y}$.\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n |\\hat{y}^{(i)} - y^{(i)}| $$\nwhere:\n$\\theta^{*}$ is the optimal(right) value of $\\theta$. $\\hat{y} = X\\theta$ represents the predicted values $y$ is the actual output vector The term $|\\hat{y}^{(i)} - y^{(i)}|$ represents the represents the error (or residual) for each data point. Modulus ensures that all errors are positive, so that underestimations and overestimations don’t cancel each other out. But, in practice, we DO NOT use the above equation. We use,\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$\nThis is because:\nMathematical convenience: The squared function is differentiable everywhere, making it easier to differentiate and compute derivatives compared to the modulus function which is not differentiable at it\u0026rsquo;s minimum (at zero). Square function vs modulus function\nNow, in Machine Learning, a Loss Function measures how well our model\u0026rsquo;s predictions match the actual values. It quantifies the error between the predicted output $\\hat{y}$ and the actual output $y$.\nLet\u0026rsquo;s represent Loss function with $J(\\theta)$. That implies,\n$$ \\implies J(\\theta) = \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$ $$ \\implies J(\\theta) = ||\\hat{y} - y||_2 $$ $$ \\theta^{*} = \\underset{\\theta}{argmin} J(\\theta) $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nCalifornia Housing Dataset\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ h_{\\theta}(x^{(i)}) = \\theta_{1} x_1^{(i)} + \\theta_{2} x_2^{(i)} + \\theta_{3} x_3^{(i)} + \\dots + \\theta_{d} x_d^{(i)} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n$x^{(i)}$ is of the shape $(1,d)$ and $θ$ is of the shape $(d,1)$ (you can see that the shapes are compatible and you can do matrix multiplication)\n$h_θ(x^{(i)})$ is of the shape $(1,1)$ which is nothing but a scalar and it is what you expect because it is the price of a single house.\nNow, we can stack all the examples (or rows) together and write it in a compact matrix form like,\n$$ \\hat{y} = h_{\\theta}(X) = X \\theta $$\n$X$ is of the shape $(n,d)$ and $θ$ is of the shape $(d,1)$.\n$h_θ(X)$ and $\\hat{y}$ are of the shape $(n,1)$ which are the prices of $n$ houses stored in a vector.\nLoss Function From the above discussion, you must\u0026rsquo;ve figured out that the goal is to find the right value of $\\theta$. That is, the $\\theta$ which gives the correct value or value as close as possible to $y$, given $X$.\nTherefore, you can say that if you\u0026rsquo;re predicted vector $\\hat{y}$ is close to the actual vector $y$, then you are proceding in the right direction with respect to $\\theta$.\nSo, the goal is to minimize the difference between $y$ and $\\hat{y}$.\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n |\\hat{y}^{(i)} - y^{(i)}| $$\nwhere:\n$\\theta^{*}$ is the optimal(right) value of $\\theta$. $\\hat{y} = X\\theta$ represents the predicted values $y$ is the actual output vector The term $|\\hat{y}^{(i)} - y^{(i)}|$ represents the represents the error (or residual) for each data point. Modulus ensures that all errors are positive, so that underestimations and overestimations don’t cancel each other out. But, in practice, we DO NOT use the above equation. We use,\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$\nThis is because:\nMathematical convenience: The squared function is differentiable everywhere, making it easier to differentiate and compute derivatives compared to the modulus function which is not differentiable at it\u0026rsquo;s minimum (at zero). Square function vs modulus function\nNow, in Machine Learning, a Loss Function measures how well our model\u0026rsquo;s predictions match the actual values. It quantifies the error between the predicted output $\\hat{y}$ and the actual output $y$.\nLet\u0026rsquo;s represent Loss function with $J(\\theta)$. That implies,\n$$ \\implies J(\\theta) = \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$ $$ \\implies J(\\theta) = ||\\hat{y} - y||_2 $$ $$ \\implies \\theta^{*} = \\underset{\\theta}{argmin} J(\\theta) $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nCalifornia Housing Dataset\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ h_{\\theta}(x^{(i)}) = \\theta_{1} x_1^{(i)} + \\theta_{2} x_2^{(i)} + \\theta_{3} x_3^{(i)} + \\dots + \\theta_{d} x_d^{(i)} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n$x^{(i)}$ is of the shape $(1,d)$ and $θ$ is of the shape $(d,1)$ (you can see that the shapes are compatible and you can do matrix multiplication)\n$h_θ(x^{(i)})$ is of the shape $(1,1)$ which is nothing but a scalar and it is what you expect because it is the price of a single house.\nNow, we can stack all the examples (or rows) together and write it in a compact matrix form like,\n$$ \\hat{y} = h_{\\theta}(X) = X \\theta $$\n$X$ is of the shape $(n,d)$ and $θ$ is of the shape $(d,1)$.\n$h_θ(X)$ and $\\hat{y}$ are of the shape $(n,1)$ which are the prices of $n$ houses stored in a vector.\nLoss Function From the above discussion, you must\u0026rsquo;ve figured out that the goal is to find the right value of $\\theta$. That is, the $\\theta$ which gives the correct value or value as close as possible to $y$, given $X$.\nTherefore, you can say that if you\u0026rsquo;re predicted vector $\\hat{y}$ is close to the actual vector $y$, then you are proceding in the right direction with respect to $\\theta$.\nSo, the goal is to minimize the difference between $y$ and $\\hat{y}$.\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n |\\hat{y}^{(i)} - y^{(i)}| $$\nwhere:\n$\\theta^{*}$ is the optimal(right) value of $\\theta$. $\\hat{y} = X\\theta$ represents the predicted values $y$ is the actual output vector The term $|\\hat{y}^{(i)} - y^{(i)}|$ represents the represents the error (or residual) for each data point. Modulus ensures that all errors are positive, so that underestimations and overestimations don’t cancel each other out. But, in practice, we DO NOT use the above equation. We use,\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$\nThis is because:\nMathematical convenience: The squared function is differentiable everywhere, making it easier to differentiate and compute derivatives compared to the modulus function which is not differentiable at it\u0026rsquo;s minimum (at zero). Square function vs modulus function\nNow, in Machine Learning, a Loss Function measures how well our model\u0026rsquo;s predictions match the actual values. It quantifies the error between the predicted output $\\hat{y}$ and the actual output $y$.\nLet\u0026rsquo;s represent Loss function with $J(\\theta)$. That implies,\n$$ J(\\theta) = \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$ $$ \\implies J(\\theta) = ||\\hat{y} - y||_2 $$ $$ \\implies \\theta^{*} = \\underset{\\theta}{argmin} J(\\theta) $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nCalifornia Housing Dataset\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ h_{\\theta}(x^{(i)}) = \\theta_{1} x_1^{(i)} + \\theta_{2} x_2^{(i)} + \\theta_{3} x_3^{(i)} + \\dots + \\theta_{d} x_d^{(i)} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n$x^{(i)}$ is of the shape $(1,d)$ and $θ$ is of the shape $(d,1)$ (you can see that the shapes are compatible and you can do matrix multiplication)\n$h_θ(x^{(i)})$ is of the shape $(1,1)$ which is nothing but a scalar and it is what you expect because it is the price of a single house.\nNow, we can stack all the examples (or rows) together and write it in a compact matrix form like,\n$$ \\hat{y} = h_{\\theta}(X) = X \\theta $$\n$X$ is of the shape $(n,d)$ and $θ$ is of the shape $(d,1)$.\n$h_θ(X)$ and $\\hat{y}$ are of the shape $(n,1)$ which are the prices of $n$ houses stored in a vector.\nLoss Function From the above discussion, you must\u0026rsquo;ve figured out that the goal is to find the right value of $\\theta$. That is, the $\\theta$ which gives the correct value or value as close as possible to $y$, given $X$.\nTherefore, you can say that if you\u0026rsquo;re predicted vector $\\hat{y}$ is close to the actual vector $y$, then you are proceding in the right direction with respect to $\\theta$.\nSo, the goal is to minimize the difference between $y$ and $\\hat{y}$.\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n |\\hat{y}^{(i)} - y^{(i)}| $$\nwhere:\n$\\theta^{*}$ is the optimal(right) value of $\\theta$. $\\hat{y} = X\\theta$ represents the predicted values $y$ is the actual output vector The term $|\\hat{y}^{(i)} - y^{(i)}|$ represents the represents the error (or residual) for each data point. Modulus ensures that all errors are positive, so that underestimations and overestimations don’t cancel each other out. But, in practice, we DO NOT use the above equation. We use,\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$\nThis is because:\nMathematical convenience: The squared function is differentiable everywhere, making it easier to differentiate and compute derivatives compared to the modulus function which is not differentiable at it\u0026rsquo;s minimum (at zero). Square function vs modulus function\nNow, in Machine Learning, a Loss Function measures how well our model\u0026rsquo;s predictions match the actual values. It quantifies the error between the predicted output $\\hat{y}$ and the actual output $y$.\nLet\u0026rsquo;s represent Loss function with $J(\\theta)$. That implies,\n$$ J(\\theta) = \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$ $$ J(\\theta) = ||\\hat{y} - y||_2 $$ $$ \\implies \\theta^{*} = \\underset{\\theta}{argmin} J(\\theta) $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nCalifornia Housing Dataset\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ h_{\\theta}(x^{(i)}) = \\theta_{1} x_1^{(i)} + \\theta_{2} x_2^{(i)} + \\theta_{3} x_3^{(i)} + \\dots + \\theta_{d} x_d^{(i)} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n$x^{(i)}$ is of the shape $(1,d)$ and $θ$ is of the shape $(d,1)$ (you can see that the shapes are compatible and you can do matrix multiplication)\n$h_θ(x^{(i)})$ is of the shape $(1,1)$ which is nothing but a scalar and it is what you expect because it is the price of a single house.\nNow, we can stack all the examples (or rows) together and write it in a compact matrix form like,\n$$ \\hat{y} = h_{\\theta}(X) = X \\theta $$\n$X$ is of the shape $(n,d)$ and $θ$ is of the shape $(d,1)$.\n$h_θ(X)$ and $\\hat{y}$ are of the shape $(n,1)$ which are the prices of $n$ houses stored in a vector.\nLoss Function From the above discussion, you must\u0026rsquo;ve figured out that the goal is to find the right value of $\\theta$. That is, the $\\theta$ which gives the correct value or value as close as possible to $y$, given $X$.\nTherefore, you can say that if you\u0026rsquo;re predicted vector $\\hat{y}$ is close to the actual vector $y$, then you are proceding in the right direction with respect to $\\theta$.\nSo, the goal is to minimize the difference between $y$ and $\\hat{y}$.\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n |\\hat{y}^{(i)} - y^{(i)}| $$\nwhere:\n$\\theta^{*}$ is the optimal(right) value of $\\theta$. $\\hat{y} = X\\theta$ represents the predicted values $y$ is the actual output vector The term $|\\hat{y}^{(i)} - y^{(i)}|$ represents the represents the error (or residual) for each data point. Modulus ensures that all errors are positive, so that underestimations and overestimations don’t cancel each other out. But, in practice, we DO NOT use the above equation. We use,\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$\nThis is because:\nMathematical convenience: The squared function is differentiable everywhere, making it easier to differentiate and compute derivatives compared to the modulus function which is not differentiable at it\u0026rsquo;s minimum (at zero). Square function vs modulus function\nNow, in Machine Learning, a Loss Function measures how well our model\u0026rsquo;s predictions match the actual values. It quantifies the error between the predicted output $\\hat{y}$ and the actual output $y$.\nLet\u0026rsquo;s represent Loss function with $J(\\theta)$. That implies,\n$$ J(\\theta) = \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$ $$ J(\\theta) = ||\\hat{y} - y||_2 $$ $$ \\theta^{*} = \\underset{\\theta}{argmin} J(\\theta) $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nCalifornia Housing Dataset\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ h_{\\theta}(x^{(i)}) = \\theta_{1} x_1^{(i)} + \\theta_{2} x_2^{(i)} + \\theta_{3} x_3^{(i)} + \\dots + \\theta_{d} x_d^{(i)} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n$x^{(i)}$ is of the shape $(1,d)$ and $θ$ is of the shape $(d,1)$ (you can see that the shapes are compatible and you can do matrix multiplication)\n$h_θ(x^{(i)})$ is of the shape $(1,1)$ which is nothing but a scalar and it is what you expect because it is the price of a single house.\nNow, we can stack all the examples (or rows) together and write it in a compact matrix form like,\n$$ \\hat{y} = h_{\\theta}(X) = X \\theta $$\n$X$ is of the shape $(n,d)$ and $θ$ is of the shape $(d,1)$.\n$h_θ(X)$ and $\\hat{y}$ are of the shape $(n,1)$ which are the prices of $n$ houses stored in a vector.\nLoss Function From the above discussion, you must\u0026rsquo;ve figured out that the goal is to find the right value of $\\theta$. That is, the $\\theta$ which gives the correct value or value as close as possible to $y$, given $X$.\nTherefore, you can say that if you\u0026rsquo;re predicted vector $\\hat{y}$ is close to the actual vector $y$, then you are proceding in the right direction with respect to $\\theta$.\nSo, the goal is to minimize the difference between $y$ and $\\hat{y}$.\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n |\\hat{y}^{(i)} - y^{(i)}| $$\nwhere:\n$\\theta^{*}$ is the optimal(right) value of $\\theta$. $\\hat{y} = X\\theta$ represents the predicted values $y$ is the actual output vector The term $|\\hat{y}^{(i)} - y^{(i)}|$ represents the represents the error (or residual) for each data point. Modulus ensures that all errors are positive, so that underestimations and overestimations don’t cancel each other out. But, in practice, we DO NOT use the above equation. We use,\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$\nThis is because:\nMathematical convenience: The squared function is differentiable everywhere, making it easier to differentiate and compute derivatives compared to the modulus function which is not differentiable at it\u0026rsquo;s minimum (at zero). Square function vs modulus function\nNow, in Machine Learning, a Loss Function measures how well our model\u0026rsquo;s predictions match the actual values. It quantifies the error between the predicted output $\\hat{y}$ and the actual output $y$.\nLet\u0026rsquo;s represent Loss function with $J(\\theta)$. That implies,\n$$ J(\\theta) = \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$ $$ J(\\theta) = ||\\hat{y} - y||_2 $$ $$ \\theta^{*} = \\underset{\\theta}{argmin} J(\\theta) $$\n$||\\hat{y} - y||_2$ is the $L2$ norm.\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nCalifornia Housing Dataset\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ h_{\\theta}(x^{(i)}) = \\theta_{1} x_1^{(i)} + \\theta_{2} x_2^{(i)} + \\theta_{3} x_3^{(i)} + \\dots + \\theta_{d} x_d^{(i)} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n$x^{(i)}$ is of the shape $(1,d)$ and $θ$ is of the shape $(d,1)$ (you can see that the shapes are compatible and you can do matrix multiplication)\n$h_θ(x^{(i)})$ is of the shape $(1,1)$ which is nothing but a scalar and it is what you expect because it is the price of a single house.\nNow, we can stack all the examples (or rows) together and write it in a compact matrix form like,\n$$ \\hat{y} = h_{\\theta}(X) = X \\theta $$\n$X$ is of the shape $(n,d)$ and $θ$ is of the shape $(d,1)$.\n$h_θ(X)$ and $\\hat{y}$ are of the shape $(n,1)$ which are the prices of $n$ houses stored in a vector.\nLoss Function From the above discussion, you must\u0026rsquo;ve figured out that the goal is to find the right value of $\\theta$. That is, the $\\theta$ which gives the correct value or value as close as possible to $y$, given $X$.\nTherefore, you can say that if you\u0026rsquo;re predicted vector $\\hat{y}$ is close to the actual vector $y$, then you are proceding in the right direction with respect to $\\theta$.\nSo, the goal is to minimize the difference between $y$ and $\\hat{y}$.\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n |\\hat{y}^{(i)} - y^{(i)}| $$\nwhere:\n$\\theta^{*}$ is the optimal(right) value of $\\theta$. $\\hat{y} = X\\theta$ represents the predicted values $y$ is the actual output vector The term $|\\hat{y}^{(i)} - y^{(i)}|$ represents the represents the error (or residual) for each data point. Modulus ensures that all errors are positive, so that underestimations and overestimations don’t cancel each other out. But, in practice, we DO NOT use the above equation. We use,\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$\nThis is because:\nMathematical convenience: The squared function is differentiable everywhere, making it easier to differentiate and compute derivatives compared to the modulus function which is not differentiable at it\u0026rsquo;s minimum (at zero). Square function vs modulus function\nNow, in Machine Learning, a Loss Function measures how well our model\u0026rsquo;s predictions match the actual values. It quantifies the error between the predicted output $\\hat{y}$ and the actual output $y$.\nLet\u0026rsquo;s represent Loss function with $J(\\theta)$. That implies,\n$$ J(\\theta) = \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$ $$ J(\\theta) = ||\\hat{y} - y||_2 $$ $$ \\theta^{*} = \\underset{\\theta}{argmin} J(\\theta) $$\nwhere $||\\hat{y} - y||_2$ is the $L2$ norm.\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nCalifornia Housing Dataset\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ h_{\\theta}(x^{(i)}) = \\theta_{1} x_1^{(i)} + \\theta_{2} x_2^{(i)} + \\theta_{3} x_3^{(i)} + \\dots + \\theta_{d} x_d^{(i)} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n$x^{(i)}$ is of the shape $(1,d)$ and $θ$ is of the shape $(d,1)$ (you can see that the shapes are compatible and you can do matrix multiplication)\n$h_θ(x^{(i)})$ is of the shape $(1,1)$ which is nothing but a scalar and it is what you expect because it is the price of a single house.\nNow, we can stack all the examples (or rows) together and write it in a compact matrix form like,\n$$ \\hat{y} = h_{\\theta}(X) = X \\theta $$\n$X$ is of the shape $(n,d)$ and $θ$ is of the shape $(d,1)$.\n$h_θ(X)$ and $\\hat{y}$ are of the shape $(n,1)$ which are the prices of $n$ houses stored in a vector.\nLoss Function From the above discussion, you must\u0026rsquo;ve figured out that the goal is to find the right value of $\\theta$. That is, the $\\theta$ which gives the correct value or value as close as possible to $y$, given $X$.\nTherefore, you can say that if you\u0026rsquo;re predicted vector $\\hat{y}$ is close to the actual vector $y$, then you are proceding in the right direction with respect to $\\theta$.\nSo, the goal is to minimize the difference between $y$ and $\\hat{y}$.\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n |\\hat{y}^{(i)} - y^{(i)}| $$\nwhere:\n$\\theta^{*}$ is the optimal(right) value of $\\theta$. $\\hat{y} = X\\theta$ represents the predicted values $y$ is the actual output vector The term $|\\hat{y}^{(i)} - y^{(i)}|$ represents the represents the error (or residual) for each data point. Modulus ensures that all errors are positive, so that underestimations and overestimations don’t cancel each other out. But, in practice, we DO NOT use the above equation. We use,\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$\nThis is because:\nMathematical convenience: The squared function is differentiable everywhere, making it easier to differentiate and compute derivatives compared to the modulus function which is not differentiable at it\u0026rsquo;s minimum (at zero). Square function vs modulus function\nNow, in Machine Learning, a Loss Function measures how well our model\u0026rsquo;s predictions match the actual values. It quantifies the error between the predicted output $\\hat{y}$ and the actual output $y$.\nLet\u0026rsquo;s represent Loss function with $J(\\theta)$. That implies,\n$$ J(\\theta) = \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$ $$ J(\\theta) = ||\\hat{y} - y||_2 $$ $$ \\theta^{*} = \\underset{\\theta}{argmin} J(\\theta) $$\nwhere $||\\hat{y} - y||_2$ is the $L2$ norm (vector representation).\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nCalifornia Housing Dataset\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ h_{\\theta}(x^{(i)}) = \\theta_{1} x_1^{(i)} + \\theta_{2} x_2^{(i)} + \\theta_{3} x_3^{(i)} + \\dots + \\theta_{d} x_d^{(i)} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n$x^{(i)}$ is of the shape $(1,d)$ and $θ$ is of the shape $(d,1)$ (you can see that the shapes are compatible and you can do matrix multiplication)\n$h_θ(x^{(i)})$ is of the shape $(1,1)$ which is nothing but a scalar and it is what you expect because it is the price of a single house.\nNow, we can stack all the examples (or rows) together and write it in a compact matrix form like,\n$$ \\hat{y} = h_{\\theta}(X) = X \\theta $$\n$X$ is of the shape $(n,d)$ and $θ$ is of the shape $(d,1)$.\n$h_θ(X)$ and $\\hat{y}$ are of the shape $(n,1)$ which are the prices of $n$ houses stored in a vector.\nLoss Function From the above discussion, you must\u0026rsquo;ve figured out that the goal is to find the right value of $\\theta$. That is, the $\\theta$ which gives the correct value or value as close as possible to $y$, given $X$.\nTherefore, you can say that if you\u0026rsquo;re predicted vector $\\hat{y}$ is close to the actual vector $y$, then you are proceding in the right direction with respect to $\\theta$.\nSo, the goal is to minimize the difference between $y$ and $\\hat{y}$.\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n |\\hat{y}^{(i)} - y^{(i)}| $$\nwhere:\n$\\theta^{*}$ is the optimal(right) value of $\\theta$. $\\hat{y} = X\\theta$ represents the predicted values $y$ is the actual output vector The term $|\\hat{y}^{(i)} - y^{(i)}|$ represents the represents the error (or residual) for each data point. Modulus ensures that all errors are positive, so that underestimations and overestimations don’t cancel each other out. But, in practice, we DO NOT use the above equation. We use,\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$\nThis is because:\nMathematical convenience: The squared function is differentiable everywhere, making it easier to differentiate and compute derivatives compared to the modulus function which is not differentiable at it\u0026rsquo;s minimum (at zero). Square function vs modulus function\nNow, in Machine Learning, a Loss Function measures how well our model\u0026rsquo;s predictions match the actual values. It quantifies the error between the predicted output $\\hat{y}$ and the actual output $y$.\nLet\u0026rsquo;s represent Loss function with $J(\\theta)$. That implies,\n$$ J(\\theta) = \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$ $$ J(\\theta) = ||\\hat{y} - y||_2 $$ $$ \\theta^{*} = \\underset{\\theta}{argmin} J(\\theta) $$\nwhere $||\\hat{y} - y||_2$ is the $L2$ norm (vector representation). This is also called the Least Squares Error.\nNote: Here, we discussed about loss function of Linear Regression intuiti\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nCalifornia Housing Dataset\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ h_{\\theta}(x^{(i)}) = \\theta_{1} x_1^{(i)} + \\theta_{2} x_2^{(i)} + \\theta_{3} x_3^{(i)} + \\dots + \\theta_{d} x_d^{(i)} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n$x^{(i)}$ is of the shape $(1,d)$ and $θ$ is of the shape $(d,1)$ (you can see that the shapes are compatible and you can do matrix multiplication)\n$h_θ(x^{(i)})$ is of the shape $(1,1)$ which is nothing but a scalar and it is what you expect because it is the price of a single house.\nNow, we can stack all the examples (or rows) together and write it in a compact matrix form like,\n$$ \\hat{y} = h_{\\theta}(X) = X \\theta $$\n$X$ is of the shape $(n,d)$ and $θ$ is of the shape $(d,1)$.\n$h_θ(X)$ and $\\hat{y}$ are of the shape $(n,1)$ which are the prices of $n$ houses stored in a vector.\nLoss Function From the above discussion, you must\u0026rsquo;ve figured out that the goal is to find the right value of $\\theta$. That is, the $\\theta$ which gives the correct value or value as close as possible to $y$, given $X$.\nTherefore, you can say that if you\u0026rsquo;re predicted vector $\\hat{y}$ is close to the actual vector $y$, then you are proceding in the right direction with respect to $\\theta$.\nSo, the goal is to minimize the difference between $y$ and $\\hat{y}$.\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n |\\hat{y}^{(i)} - y^{(i)}| $$\nwhere:\n$\\theta^{*}$ is the optimal(right) value of $\\theta$. $\\hat{y} = X\\theta$ represents the predicted values $y$ is the actual output vector The term $|\\hat{y}^{(i)} - y^{(i)}|$ represents the represents the error (or residual) for each data point. Modulus ensures that all errors are positive, so that underestimations and overestimations don’t cancel each other out. But, in practice, we DO NOT use the above equation. We use,\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$\nThis is because:\nMathematical convenience: The squared function is differentiable everywhere, making it easier to differentiate and compute derivatives compared to the modulus function which is not differentiable at it\u0026rsquo;s minimum (at zero). Square function vs modulus function\nNow, in Machine Learning, a Loss Function measures how well our model\u0026rsquo;s predictions match the actual values. It quantifies the error between the predicted output $\\hat{y}$ and the actual output $y$.\nLet\u0026rsquo;s represent Loss function with $J(\\theta)$. That implies,\n$$ J(\\theta) = \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$ $$ J(\\theta) = ||\\hat{y} - y||_2 $$ $$ \\theta^{*} = \\underset{\\theta}{argmin} J(\\theta) $$\nwhere $||\\hat{y} - y||_2$ is the $L2$ norm (vector representation). This is also called the Least Squared Error.\nNote: Here, we discussed about loss function of Linear Regression intuiti\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nCalifornia Housing Dataset\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ h_{\\theta}(x^{(i)}) = \\theta_{1} x_1^{(i)} + \\theta_{2} x_2^{(i)} + \\theta_{3} x_3^{(i)} + \\dots + \\theta_{d} x_d^{(i)} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n$x^{(i)}$ is of the shape $(1,d)$ and $θ$ is of the shape $(d,1)$ (you can see that the shapes are compatible and you can do matrix multiplication)\n$h_θ(x^{(i)})$ is of the shape $(1,1)$ which is nothing but a scalar and it is what you expect because it is the price of a single house.\nNow, we can stack all the examples (or rows) together and write it in a compact matrix form like,\n$$ \\hat{y} = h_{\\theta}(X) = X \\theta $$\n$X$ is of the shape $(n,d)$ and $θ$ is of the shape $(d,1)$.\n$h_θ(X)$ and $\\hat{y}$ are of the shape $(n,1)$ which are the prices of $n$ houses stored in a vector.\nLoss Function From the above discussion, you must\u0026rsquo;ve figured out that the goal is to find the right value of $\\theta$. That is, the $\\theta$ which gives the correct value or value as close as possible to $y$, given $X$.\nTherefore, you can say that if you\u0026rsquo;re predicted vector $\\hat{y}$ is close to the actual vector $y$, then you are proceding in the right direction with respect to $\\theta$.\nSo, the goal is to minimize the difference between $y$ and $\\hat{y}$.\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n |\\hat{y}^{(i)} - y^{(i)}| $$\nwhere:\n$\\theta^{*}$ is the optimal(right) value of $\\theta$. $\\hat{y} = X\\theta$ represents the predicted values $y$ is the actual output vector The term $|\\hat{y}^{(i)} - y^{(i)}|$ represents the represents the error (or residual) for each data point. Modulus ensures that all errors are positive, so that underestimations and overestimations don’t cancel each other out. But, in practice, we DO NOT use the above equation. We use,\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$\nThis is because:\nMathematical convenience: The squared function is differentiable everywhere, making it easier to differentiate and compute derivatives compared to the modulus function which is not differentiable at it\u0026rsquo;s minimum (at zero). Square function vs modulus function\nNow, in Machine Learning, a Loss Function measures how well our model\u0026rsquo;s predictions match the actual values. It quantifies the error between the predicted output $\\hat{y}$ and the actual output $y$.\nLet\u0026rsquo;s represent Loss function with $J(\\theta)$. That implies,\n$$ J(\\theta) = \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$ $$ J(\\theta) = ||\\hat{y} - y||_2 $$ $$ \\theta^{*} = \\underset{\\theta}{argmin} J(\\theta) $$\nwhere $||\\hat{y} - y||_2$ is the $L2$ norm (vector representation). This loss function is also called the Least Squared Error.\nNote: Here, we discussed about loss function of Linear Regression intuiti\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nCalifornia Housing Dataset\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ h_{\\theta}(x^{(i)}) = \\theta_{1} x_1^{(i)} + \\theta_{2} x_2^{(i)} + \\theta_{3} x_3^{(i)} + \\dots + \\theta_{d} x_d^{(i)} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n$x^{(i)}$ is of the shape $(1,d)$ and $θ$ is of the shape $(d,1)$ (you can see that the shapes are compatible and you can do matrix multiplication)\n$h_θ(x^{(i)})$ is of the shape $(1,1)$ which is nothing but a scalar and it is what you expect because it is the price of a single house.\nNow, we can stack all the examples (or rows) together and write it in a compact matrix form like,\n$$ \\hat{y} = h_{\\theta}(X) = X \\theta $$\n$X$ is of the shape $(n,d)$ and $θ$ is of the shape $(d,1)$.\n$h_θ(X)$ and $\\hat{y}$ are of the shape $(n,1)$ which are the prices of $n$ houses stored in a vector.\nLoss Function From the above discussion, you must\u0026rsquo;ve figured out that the goal is to find the right value of $\\theta$. That is, the $\\theta$ which gives the correct value or value as close as possible to $y$, given $X$.\nTherefore, you can say that if you\u0026rsquo;re predicted vector $\\hat{y}$ is close to the actual vector $y$, then you are proceding in the right direction with respect to $\\theta$.\nSo, the goal is to minimize the difference between $y$ and $\\hat{y}$.\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n |\\hat{y}^{(i)} - y^{(i)}| $$\nwhere:\n$\\theta^{*}$ is the optimal(right) value of $\\theta$. $\\hat{y} = X\\theta$ represents the predicted values $y$ is the actual output vector The term $|\\hat{y}^{(i)} - y^{(i)}|$ represents the represents the error (or residual) for each data point. Modulus ensures that all errors are positive, so that underestimations and overestimations don’t cancel each other out. But, in practice, we DO NOT use the above equation. We use,\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$\nThis is because:\nMathematical convenience: The squared function is differentiable everywhere, making it easier to differentiate and compute derivatives compared to the modulus function which is not differentiable at it\u0026rsquo;s minimum (at zero). Square function vs modulus function\nNow, in Machine Learning, a Loss Function measures how well our model\u0026rsquo;s predictions match the actual values. It quantifies the error between the predicted output $\\hat{y}$ and the actual output $y$.\nLet\u0026rsquo;s represent Loss function with $J(\\theta)$. That implies,\n$$ J(\\theta) = \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$ $$ J(\\theta) = ||\\hat{y} - y||_2 $$ $$ \\theta^{*} = \\underset{\\theta}{argmin} J(\\theta) $$\nwhere $||\\hat{y} - y||_2$ is the $L2$ norm (vector representation).\nThe above loss function is also called the Least Squared Error.\nNote: Here, we discussed about loss function of Linear Regression intuiti\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nCalifornia Housing Dataset\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ h_{\\theta}(x^{(i)}) = \\theta_{1} x_1^{(i)} + \\theta_{2} x_2^{(i)} + \\theta_{3} x_3^{(i)} + \\dots + \\theta_{d} x_d^{(i)} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n$x^{(i)}$ is of the shape $(1,d)$ and $θ$ is of the shape $(d,1)$ (you can see that the shapes are compatible and you can do matrix multiplication)\n$h_θ(x^{(i)})$ is of the shape $(1,1)$ which is nothing but a scalar and it is what you expect because it is the price of a single house.\nNow, we can stack all the examples (or rows) together and write it in a compact matrix form like,\n$$ \\hat{y} = h_{\\theta}(X) = X \\theta $$\n$X$ is of the shape $(n,d)$ and $θ$ is of the shape $(d,1)$.\n$h_θ(X)$ and $\\hat{y}$ are of the shape $(n,1)$ which are the prices of $n$ houses stored in a vector.\nLoss Function From the above discussion, you must\u0026rsquo;ve figured out that the goal is to find the right value of $\\theta$. That is, the $\\theta$ which gives the correct value or value as close as possible to $y$, given $X$.\nTherefore, you can say that if you\u0026rsquo;re predicted vector $\\hat{y}$ is close to the actual vector $y$, then you are proceding in the right direction with respect to $\\theta$.\nSo, the goal is to minimize the difference between $y$ and $\\hat{y}$.\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n |\\hat{y}^{(i)} - y^{(i)}| $$\nwhere:\n$\\theta^{*}$ is the optimal(right) value of $\\theta$. $\\hat{y} = X\\theta$ represents the predicted values $y$ is the actual output vector The term $|\\hat{y}^{(i)} - y^{(i)}|$ represents the represents the error (or residual) for each data point. Modulus ensures that all errors are positive, so that underestimations and overestimations don’t cancel each other out. But, in practice, we DO NOT use the above equation. We use,\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$\nThis is because:\nMathematical convenience: The squared function is differentiable everywhere, making it easier to differentiate and compute derivatives compared to the modulus function which is not differentiable at it\u0026rsquo;s minimum (at zero). Square function vs modulus function\nNow, in Machine Learning, a Loss Function measures how well our model\u0026rsquo;s predictions match the actual values. It quantifies the error between the predicted output $\\hat{y}$ and the actual output $y$.\nLet\u0026rsquo;s represent Loss function with $J(\\theta)$. That implies,\n$$ J(\\theta) = \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$ $$ J(\\theta) = ||\\hat{y} - y||_2 $$ $$ \\theta^{*} = \\underset{\\theta}{argmin} J(\\theta) $$\nwhere $||\\hat{y} - y||_2$ is the $L2$ norm (vector representation).\nThe above loss function is also called the Least Squared Error.\nNote Note: Here, we discussed about loss function of Linear Regression intuiti\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nCalifornia Housing Dataset\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ h_{\\theta}(x^{(i)}) = \\theta_{1} x_1^{(i)} + \\theta_{2} x_2^{(i)} + \\theta_{3} x_3^{(i)} + \\dots + \\theta_{d} x_d^{(i)} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n$x^{(i)}$ is of the shape $(1,d)$ and $θ$ is of the shape $(d,1)$ (you can see that the shapes are compatible and you can do matrix multiplication)\n$h_θ(x^{(i)})$ is of the shape $(1,1)$ which is nothing but a scalar and it is what you expect because it is the price of a single house.\nNow, we can stack all the examples (or rows) together and write it in a compact matrix form like,\n$$ \\hat{y} = h_{\\theta}(X) = X \\theta $$\n$X$ is of the shape $(n,d)$ and $θ$ is of the shape $(d,1)$.\n$h_θ(X)$ and $\\hat{y}$ are of the shape $(n,1)$ which are the prices of $n$ houses stored in a vector.\nLoss Function From the above discussion, you must\u0026rsquo;ve figured out that the goal is to find the right value of $\\theta$. That is, the $\\theta$ which gives the correct value or value as close as possible to $y$, given $X$.\nTherefore, you can say that if you\u0026rsquo;re predicted vector $\\hat{y}$ is close to the actual vector $y$, then you are proceding in the right direction with respect to $\\theta$.\nSo, the goal is to minimize the difference between $y$ and $\\hat{y}$.\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n |\\hat{y}^{(i)} - y^{(i)}| $$\nwhere:\n$\\theta^{*}$ is the optimal(right) value of $\\theta$. $\\hat{y} = X\\theta$ represents the predicted values $y$ is the actual output vector The term $|\\hat{y}^{(i)} - y^{(i)}|$ represents the represents the error (or residual) for each data point. Modulus ensures that all errors are positive, so that underestimations and overestimations don’t cancel each other out. But, in practice, we DO NOT use the above equation. We use,\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$\nThis is because:\nMathematical convenience: The squared function is differentiable everywhere, making it easier to differentiate and compute derivatives compared to the modulus function which is not differentiable at it\u0026rsquo;s minimum (at zero). Square function vs modulus function\nNow, in Machine Learning, a Loss Function measures how well our model\u0026rsquo;s predictions match the actual values. It quantifies the error between the predicted output $\\hat{y}$ and the actual output $y$.\nLet\u0026rsquo;s represent Loss function with $J(\\theta)$. That implies,\n$$ J(\\theta) = \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$ $$ J(\\theta) = ||\\hat{y} - y||_2 $$ $$ \\theta^{*} = \\underset{\\theta}{argmin} J(\\theta) $$\nwhere $||\\hat{y} - y||_2$ is the $L2$ norm (vector representation).\nThe above loss function is also called the Least Squared Error.\nNote Note: Here, we discussed about loss function of Linear Regression intuiti\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nCalifornia Housing Dataset\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ h_{\\theta}(x^{(i)}) = \\theta_{1} x_1^{(i)} + \\theta_{2} x_2^{(i)} + \\theta_{3} x_3^{(i)} + \\dots + \\theta_{d} x_d^{(i)} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n$x^{(i)}$ is of the shape $(1,d)$ and $θ$ is of the shape $(d,1)$ (you can see that the shapes are compatible and you can do matrix multiplication)\n$h_θ(x^{(i)})$ is of the shape $(1,1)$ which is nothing but a scalar and it is what you expect because it is the price of a single house.\nNow, we can stack all the examples (or rows) together and write it in a compact matrix form like,\n$$ \\hat{y} = h_{\\theta}(X) = X \\theta $$\n$X$ is of the shape $(n,d)$ and $θ$ is of the shape $(d,1)$.\n$h_θ(X)$ and $\\hat{y}$ are of the shape $(n,1)$ which are the prices of $n$ houses stored in a vector.\nLoss Function From the above discussion, you must\u0026rsquo;ve figured out that the goal is to find the right value of $\\theta$. That is, the $\\theta$ which gives the correct value or value as close as possible to $y$, given $X$.\nTherefore, you can say that if you\u0026rsquo;re predicted vector $\\hat{y}$ is close to the actual vector $y$, then you are proceding in the right direction with respect to $\\theta$.\nSo, the goal is to minimize the difference between $y$ and $\\hat{y}$.\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n |\\hat{y}^{(i)} - y^{(i)}| $$\nwhere:\n$\\theta^{*}$ is the optimal(right) value of $\\theta$. $\\hat{y} = X\\theta$ represents the predicted values $y$ is the actual output vector The term $|\\hat{y}^{(i)} - y^{(i)}|$ represents the represents the error (or residual) for each data point. Modulus ensures that all errors are positive, so that underestimations and overestimations don’t cancel each other out. But, in practice, we DO NOT use the above equation. We use,\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$\nThis is because:\nMathematical convenience: The squared function is differentiable everywhere, making it easier to differentiate and compute derivatives compared to the modulus function which is not differentiable at it\u0026rsquo;s minimum (at zero). Square function vs modulus function\nNow, in Machine Learning, a Loss Function measures how well our model\u0026rsquo;s predictions match the actual values. It quantifies the error between the predicted output $\\hat{y}$ and the actual output $y$.\nLet\u0026rsquo;s represent Loss function with $J(\\theta)$. That implies,\n$$ J(\\theta) = \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$ $$ J(\\theta) = ||\\hat{y} - y||_2 $$ $$ \\theta^{*} = \\underset{\\theta}{argmin} J(\\theta) $$\nwhere $||\\hat{y} - y||_2$ is the $L2$ norm (vector representation).\nThe above loss function is also called the Least Squared Error.\nNote Here, we discussed about loss function of Linear Regression intuiti\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nCalifornia Housing Dataset\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ h_{\\theta}(x^{(i)}) = \\theta_{1} x_1^{(i)} + \\theta_{2} x_2^{(i)} + \\theta_{3} x_3^{(i)} + \\dots + \\theta_{d} x_d^{(i)} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n$x^{(i)}$ is of the shape $(1,d)$ and $θ$ is of the shape $(d,1)$ (you can see that the shapes are compatible and you can do matrix multiplication)\n$h_θ(x^{(i)})$ is of the shape $(1,1)$ which is nothing but a scalar and it is what you expect because it is the price of a single house.\nNow, we can stack all the examples (or rows) together and write it in a compact matrix form like,\n$$ \\hat{y} = h_{\\theta}(X) = X \\theta $$\n$X$ is of the shape $(n,d)$ and $θ$ is of the shape $(d,1)$.\n$h_θ(X)$ and $\\hat{y}$ are of the shape $(n,1)$ which are the prices of $n$ houses stored in a vector.\nLoss Function From the above discussion, you must\u0026rsquo;ve figured out that the goal is to find the right value of $\\theta$. That is, the $\\theta$ which gives the correct value or value as close as possible to $y$, given $X$.\nTherefore, you can say that if you\u0026rsquo;re predicted vector $\\hat{y}$ is close to the actual vector $y$, then you are proceding in the right direction with respect to $\\theta$.\nSo, the goal is to minimize the difference between $y$ and $\\hat{y}$.\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n |\\hat{y}^{(i)} - y^{(i)}| $$\nwhere:\n$\\theta^{*}$ is the optimal(right) value of $\\theta$. $\\hat{y} = X\\theta$ represents the predicted values $y$ is the actual output vector The term $|\\hat{y}^{(i)} - y^{(i)}|$ represents the represents the error (or residual) for each data point. Modulus ensures that all errors are positive, so that underestimations and overestimations don’t cancel each other out. But, in practice, we DO NOT use the above equation. We use,\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$\nThis is because:\nMathematical convenience: The squared function is differentiable everywhere, making it easier to differentiate and compute derivatives compared to the modulus function which is not differentiable at it\u0026rsquo;s minimum (at zero). Square function vs modulus function\nNow, in Machine Learning, a Loss Function measures how well our model\u0026rsquo;s predictions match the actual values. It quantifies the error between the predicted output $\\hat{y}$ and the actual output $y$.\nLet\u0026rsquo;s represent Loss function with $J(\\theta)$. That implies,\n$$ J(\\theta) = \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$ $$ J(\\theta) = ||\\hat{y} - y||_2 $$ $$ \\theta^{*} = \\underset{\\theta}{argmin} J(\\theta) $$\nwhere $||\\hat{y} - y||_2$ is the $L2$ norm (vector representation).\nThe above loss function is also called the Least Squared Error.\nNote Here, we derived the loss function of Linear Regression intuitively. There\u0026rsquo;s also a more rigorous mathematical derivation which we will discuss later in the blog.\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nCalifornia Housing Dataset\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ h_{\\theta}(x^{(i)}) = \\theta_{1} x_1^{(i)} + \\theta_{2} x_2^{(i)} + \\theta_{3} x_3^{(i)} + \\dots + \\theta_{d} x_d^{(i)} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n$x^{(i)}$ is of the shape $(1,d)$ and $θ$ is of the shape $(d,1)$ (you can see that the shapes are compatible and you can do matrix multiplication)\n$h_θ(x^{(i)})$ is of the shape $(1,1)$ which is nothing but a scalar and it is what you expect because it is the price of a single house.\nNow, we can stack all the examples (or rows) together and write it in a compact matrix form like,\n$$ \\hat{y} = h_{\\theta}(X) = X \\theta $$\n$X$ is of the shape $(n,d)$ and $θ$ is of the shape $(d,1)$.\n$h_θ(X)$ and $\\hat{y}$ are of the shape $(n,1)$ which are the prices of $n$ houses stored in a vector.\nLoss Function From the above discussion, you must\u0026rsquo;ve figured out that the goal is to find the right value of $\\theta$. That is, the $\\theta$ which gives the correct value or value as close as possible to $y$, given $X$.\nTherefore, you can say that if you\u0026rsquo;re predicted vector $\\hat{y}$ is close to the actual vector $y$, then you are proceding in the right direction with respect to $\\theta$.\nSo, the goal is to minimize the difference between $y$ and $\\hat{y}$.\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n |\\hat{y}^{(i)} - y^{(i)}| $$\nwhere:\n$\\theta^{*}$ is the optimal(right) value of $\\theta$. $\\hat{y} = X\\theta$ represents the predicted values $y$ is the actual output vector The term $|\\hat{y}^{(i)} - y^{(i)}|$ represents the represents the error (or residual) for each data point. Modulus ensures that all errors are positive, so that underestimations and overestimations don’t cancel each other out. But, in practice, we DO NOT use the above equation. We use,\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$\nThis is because:\nMathematical convenience: The squared function is differentiable everywhere, making it easier to differentiate and compute derivatives compared to the modulus function which is not differentiable at it\u0026rsquo;s minimum (at zero). Square function vs modulus function\nNow, in Machine Learning, a Loss Function measures how well our model\u0026rsquo;s predictions match the actual values. It quantifies the error between the predicted output $\\hat{y}$ and the actual output $y$.\nLet\u0026rsquo;s represent Loss function with $J(\\theta)$. That implies,\n$$ J(\\theta) = \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$ $$ J(\\theta) = ||\\hat{y} - y||_2 $$ $$ \\theta^{*} = \\underset{\\theta}{argmin} J(\\theta) $$\nwhere $||\\hat{y} - y||_2$ is the $L2$ norm (vector representation).\nThe above loss function is also called the Least Squared Error.\nNote Here, we derived the loss function of Linear Regression intuitively. There\u0026rsquo;s also a more rigorous mathematical derivation which we will discuss later in the blog.\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nCalifornia Housing Dataset\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ h_{\\theta}(x^{(i)}) = \\theta_{1} x_1^{(i)} + \\theta_{2} x_2^{(i)} + \\theta_{3} x_3^{(i)} + \\dots + \\theta_{d} x_d^{(i)} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n$x^{(i)}$ is of the shape $(1,d)$ and $θ$ is of the shape $(d,1)$ (you can see that the shapes are compatible and you can do matrix multiplication)\n$h_θ(x^{(i)})$ is of the shape $(1,1)$ which is nothing but a scalar and it is what you expect because it is the price of a single house.\nNow, we can stack all the examples (or rows) together and write it in a compact matrix form like,\n$$ \\hat{y} = h_{\\theta}(X) = X \\theta $$\n$X$ is of the shape $(n,d)$ and $θ$ is of the shape $(d,1)$.\n$h_θ(X)$ and $\\hat{y}$ are of the shape $(n,1)$ which are the prices of $n$ houses stored in a vector.\nLoss Function From the above discussion, you must\u0026rsquo;ve figured out that the goal is to find the right value of $\\theta$. That is, the $\\theta$ which gives the correct value or value as close as possible to $y$, given $X$.\nTherefore, you can say that if you\u0026rsquo;re predicted vector $\\hat{y}$ is close to the actual vector $y$, then you are proceding in the right direction with respect to $\\theta$.\nSo, the goal is to minimize the difference between $y$ and $\\hat{y}$.\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n |\\hat{y}^{(i)} - y^{(i)}| $$\nwhere:\n$\\theta^{*}$ is the optimal(right) value of $\\theta$. $\\hat{y} = X\\theta$ represents the predicted values $y$ is the actual output vector The term $|\\hat{y}^{(i)} - y^{(i)}|$ represents the represents the error (or residual) for each data point. Modulus ensures that all errors are positive, so that underestimations and overestimations don’t cancel each other out. But, in practice, we DO NOT use the above equation. We use,\n$$ \\theta^{*} = \\underset{\\theta}{argmin} \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$\nThis is because:\nMathematical convenience: The squared function is differentiable everywhere, making it easier to differentiate and compute derivatives compared to the modulus function which is not differentiable at it\u0026rsquo;s minimum (at zero). Square function vs modulus function\nNow, in Machine Learning, a Loss Function measures how well our model\u0026rsquo;s predictions match the actual values. It quantifies the error between the predicted output $\\hat{y}$ and the actual output $y$.\nLet\u0026rsquo;s represent Loss function with $J(\\theta)$. That implies,\n$$ J(\\theta) = \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2 $$ $$ J(\\theta) = ||\\hat{y} - y||_2 $$ $$ \\theta^{*} = \\underset{\\theta}{argmin} J(\\theta) $$\nwhere $||\\hat{y} - y||_2$ is the $L2$ norm (vector representation).\nThe above loss function is also called the Least Squared Error.\nNote: Here, we derived the loss function of Linear Regression intuitively. There\u0026rsquo;s also a more rigorous mathematical derivation which we will discuss later in the blog.\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"}]