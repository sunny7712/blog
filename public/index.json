[{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are n examples, we have n such values, one for each row. We denote this by y.\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of X is (n,d). The shape of y is (n,1). ","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. ","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. ","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ \\begin{aligned} h_{\\theta}(x^{(i)}) \u0026amp;= {\\theta}{1}x^{(i)}{1} + {\\theta}{2}x^{(i)}{2} + {\\theta}{3}x^{(i)}{3} + \\dots + {\\theta}{d}x^{(i)}{d} \\end{aligned} $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ \\begin{aligned} h_{\\theta}(x^{(i)}) \u0026amp;= {\\theta}{1}x^{(i)}{1} + {\\theta}{2}x^{(i)}{2} + {\\theta}{3}x^{(i)}{3} + \\dots + {\\theta}{d}x^{(i)}{d} \\end{aligned} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \u0026hellip; x_d^{(i)}$ are are the features of the house (e.g., number of bedrooms, house age). ","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ \\begin{aligned} h_{\\theta}(x^{(i)}) \u0026amp;= {\\theta}{1}x^{(i)}{1} + {\\theta}{2}x^{(i)}{2} + {\\theta}{3}x^{(i)}{3} + \\dots + {\\theta}{d}x^{(i)}{d} \\end{aligned} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \u0026hellip; x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). ","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ \\begin{aligned} h_{\\theta}(x^{(i)}) \u0026amp;= {\\theta}{1}x^{(i)}{1} + {\\theta}{2}x^{(i)}{2} + {\\theta}{3}x^{(i)}{3} + \\dots + {\\theta}{d}x^{(i)}{d} \\end{aligned} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \u0026hellip; x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). ","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ \\begin{aligned} h_{\\theta}(x^{(i)}) \u0026amp;= {\\theta}{1}x^{(i)}{1} + {\\theta}{2}x^{(i)}{2} + {\\theta}{3}x^{(i)}{3} + \\dots + {\\theta}{d}x^{(i)}{d} \\end{aligned} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \u0026hellip; x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \u0026hellip; θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. ","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ \\begin{aligned} h_{\\theta}(x^{(i)}) \u0026amp;= {\\theta}{1}x^{(i)}{1} + {\\theta}{2}x^{(i)}{2} + {\\theta}{3}x^{(i)}{3} + \\dots + {\\theta}{d}x^{(i)}{d} \\end{aligned} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \u0026hellip; x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \u0026hellip; θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. ","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ \\begin{aligned} h_{\\theta}(x^{(i)}) \u0026amp;= {\\theta}{1}x^{(i)}{1} + {\\theta}{2}x^{(i)}{2} + {\\theta}{3}x^{(i)}{3} + \\dots + {\\theta}{d}x^{(i)}{d} \\end{aligned} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \u0026hellip; x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \u0026hellip; θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. ","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ \\begin{aligned} h_{\\theta}(x^{(i)}) \u0026amp;= {\\theta}{1}x^{(i)}{1} + {\\theta}{2}x^{(i)}{2} + {\\theta}{3}x^{(i)}{3} + \\dots + {\\theta}{d}x^{(i)}{d} \\end{aligned} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \u0026hellip; x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \u0026hellip; θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ \\begin{aligned}\nh_{\\theta}(x^{(i)}) \u0026amp;= {\\theta}{1}x^{(i)}{1} + {\\theta}{2}x^{(i)}{2} + {\\theta}{3}x^{(i)}{3} + \\dots + {\\theta}{d}x^{(i)}{d}\n\\end{aligned} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \u0026hellip; x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \u0026hellip; θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$\nh_{\\theta}(x^{(i)}) \u0026amp;= {\\theta}{1}x^{(i)}{1} + {\\theta}{2}x^{(i)}{2} + {\\theta}{3}x^{(i)}{3} + \\dots + {\\theta}{d}x^{(i)}{d}\n$$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \u0026hellip; x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \u0026hellip; θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ \\begin{aligned}\nh_{\\theta}(x^{(i)}) \u0026amp;= {\\theta}{1}x^{(i)}{1} + {\\theta}{2}x^{(i)}{2} + {\\theta}{3}x^{(i)}{3} + \\dots + {\\theta}{d}x^{(i)}{d}\n\\end{aligned} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \u0026hellip; x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \u0026hellip; θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ \\begin{aligned}\nh_{\\theta}(x^{(i)}) \u0026amp;= {\\theta}{1}x^{(i)}{1} + {\\theta}{2}x^{(i)}{2} + {\\theta}{3}x^{(i)}{3} + \\dots + {\\theta}{d}x^{(i)}{d}\n\\end{aligned}\n$$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \u0026hellip; x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \u0026hellip; θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ h_{\\theta}(x^{(i)}) = {\\theta}{1}x^{(i)}{1} + {\\theta}{2}x^{(i)}{2} + {\\theta}{3}x^{(i)}{3} + \\dots + {\\theta}{d}x^{(i)}{d} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \u0026hellip; x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \u0026hellip; θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ \\begin{align} h_{\\theta}(x^{(i)}) \u0026amp;= {\\theta}{1}x^{(i)}{1} + {\\theta}{2}x^{(i)}{2} + {\\theta}{3}x^{(i)}{3} + \\dots + {\\theta}{d}x^{(i)}{d} \\end{align} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \u0026hellip; x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \u0026hellip; θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ h_{\\theta}(x^{(i)}) \u0026amp;= {\\theta}{1}x^{(i)}{1} + {\\theta}{2}x^{(i)}{2} + {\\theta}{3}x^{(i)}{3} + \\dots + {\\theta}{d}x^{(i)}{d} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \u0026hellip; x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \u0026hellip; θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ h_{\\theta}(x^{(i)}) $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \u0026hellip; x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \u0026hellip; θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ h_{\\theta}(x^{(i)}) = {\\theta}{1}x^{(i)}{1} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \u0026hellip; x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \u0026hellip; θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ h_{\\theta}(x^{(i)}) \u0026amp;= {\\theta}{1}x^{(i)}{1} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \u0026hellip; x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \u0026hellip; θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ h_{\\theta}(x^{(i)}) $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \u0026hellip; x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \u0026hellip; θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ {h_{\\theta}(x^{(i)})} = $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \u0026hellip; x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \u0026hellip; θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ {h_{\\theta}(x^{(i)})} = {\\theta}{1}x^{(i)}{1} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \u0026hellip; x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \u0026hellip; θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ {h_{\\theta}(x^{(i)})} = {{\\theta}{1}x^{(i)}{1}} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \u0026hellip; x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \u0026hellip; θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ {h_{\\theta}(x^{(i)})} = $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \u0026hellip; x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \u0026hellip; θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ {h_{\\theta}(x^{(i)})} \u0026amp;= $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \u0026hellip; x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \u0026hellip; θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ {h_{\\theta}(x^{(i)})} \u0026amp;= {{\\theta}{1}x^{(i)}{1}} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \u0026hellip; x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \u0026hellip; θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ {h_{\\theta}(x^{(i)})} = $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \u0026hellip; x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \u0026hellip; θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ {h_{\\theta}(x^{(i)})} = x^{(i)} {\\theta} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \u0026hellip; x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \u0026hellip; θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ {h_{\\theta}(x^{(i)})} = x^{(i)} {\\theta} + x^{(i)} {\\theta} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \u0026hellip; x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \u0026hellip; θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ {h_{\\theta}(x^{(i)})} = x^{(i)} {\\theta} + x^{(i)} {\\theta}_{1} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \u0026hellip; x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \u0026hellip; θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ {h_{\\theta}(x^{(i)})} = {{\\theta}{1}} x^{(i)}{1} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \u0026hellip; x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \u0026hellip; θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ {h_{\\theta}(x^{(i)})} = {{\\theta}{1}} {x^{(i)}{1}} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \u0026hellip; x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \u0026hellip; θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ {h_{\\theta}(x^{(i)})} = {{\\theta}{1}} {x^{(i)}{1}} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \u0026hellip; x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \u0026hellip; θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ {h_{\\theta}(x^{(i)})} = {x^{(i)}_{1}} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \u0026hellip; x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \u0026hellip; θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ {h_{\\theta}(x^{(i)})} = {{\\theta}_{1}} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \u0026hellip; x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \u0026hellip; θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ {h_{\\theta}(x^{(i)})} = {{\\theta}{1}} {x^{(i)}{1}} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \u0026hellip; x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \u0026hellip; θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ {h_{\\theta}(x^{(i)})} = {{{\\theta}{1}} {x^{(i)}{1}}} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \u0026hellip; x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \u0026hellip; θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ {h_{\\theta}(x^{(i)})} = {{\\theta}{1}} * {x^{(i)}{1}} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \u0026hellip; x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \u0026hellip; θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ {h_{\\theta}(x^{(i)})} = {{\\theta}{1}} {x^{(i)}{1}} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \u0026hellip; x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \u0026hellip; θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ {h_{\\theta}(x^{(i)})} \u0026amp;= {{\\theta}{1}} {x^{(i)}{1}} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \u0026hellip; x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \u0026hellip; θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ {h_{\\theta}(x^{(i)})} = {{\\theta}{1}} {x^{(i)}{1}} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \u0026hellip; x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \u0026hellip; θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ {h_{\\theta}(x^{(i)})} = {{\\theta}{1}} {x^{(i)}{1}}\nE = mc^2 $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \u0026hellip; x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \u0026hellip; θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$\nE = mc^2 $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \u0026hellip; x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \u0026hellip; θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ E = mc^2 $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \u0026hellip; x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \u0026hellip; θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ h_{\\theta}(x^{(i)}) = {\\theta}{1}x^{(i)}{1} + {\\theta}{2}x^{(i)}{2} + {\\theta}{3}x^{(i)}{3} + \\dots + {\\theta}{d}x^{(i)}{d} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \u0026hellip; x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \u0026hellip; θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ {h_{\\theta}(x^{(i)})} = {{\\theta}{1}} {x^{(i)}{1}} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \u0026hellip; x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \u0026hellip; θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ {h_{\\theta}(x^{(i)})} = {{\\theta}_{1}} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \u0026hellip; x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \u0026hellip; θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ {h_{\\theta}(x^{(i)})} = {{\\theta}_{1}} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \u0026hellip; θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ {h_{\\theta}(x^{(i)})} = {{\\theta}_{1}} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ {h_{\\theta}(x^{(i)})} = {{\\theta}_{1} x_1^{(i)}} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ {h_{\\theta}(x^{(i)})} = {{\\theta}_{1} x_1^{(i)}} + $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ {h_{\\theta}(x^{(i)})} = {{\\theta}{1} x_1^{(i)}} + {{\\theta}{2} x_2^{(i)}} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ {h_{\\theta}(x^{(i)})} = {{\\theta}{1} x_1^{(i)}} + {{\\theta}{1} x_1^{(i)}} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ {h_{\\theta}(x^{(i)})} \u0026amp;= {{\\theta}{1} x_1^{(i)}} + {{\\theta}{1} x_1^{(i)}} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ {h_{\\theta}(x^{(i)})} = {{\\theta}{1} x_1^{(i)}} + {{\\theta}{1} x_1^{(i)}} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ {h_{\\theta}(x^{(i)})} = {{\\theta}{1} x_1^{(i)}} + {{\\theta}{1} x_1^{(i)}} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ {h_{\\theta}(x^{(i)})} = {{{\\theta}{1} x_1^{(i)}} + {{\\theta}{1} x_1^{(i)}}} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ {h_{\\theta}(x^{(i)})} = {{\\theta}{1} x_1^{(i)}} + {{\\theta}{1} x_1^{(i)}} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ {h_{\\theta}(x^{(i)})} = {{\\theta}{1} x_1^{(i)} + } + {{\\theta}{1} x_1^{(i)}} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ {h_{\\theta}(x^{(i)})} = {{\\theta}_{1} x_1^{(i)} + } + $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ {h_{\\theta}(x^{(i)})} = {{\\theta}{1} x_1^{(i)} + {\\theta}{1} x_1^{(i)}} + $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ {h_{\\theta}(x^{(i)})} = {{\\theta}{1} x_1^{(i)} + {\\theta}{1} x_1^{(i)}} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ {h_{\\theta}(x^{(i)})} = {{\\theta}_{1} x_1^{(i)} + } $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ {h_{\\theta}(x^{(i)})} = {\\theta}_{1} x_1^{(i)} +\n$$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ {h_{\\theta}(x^{(i)})} = {\\theta}{1} x_1^{(i)} + {\\theta}{1} x_1^{(i)} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ {h_{\\theta}(x^{(i)})} = {{\\theta}_{1} x_1^{(i)} + } $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ {h_{\\theta}(x^{(i)})} = {{\\theta}{1} x_1^{(i)} + {\\theta}{1} x_1^{(i)}} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ {h_{\\theta}(x^{(i)})} = {{\\theta}{1} x_1^{(i)} + {\\theta}{1} x_1^{(i)}} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ {h_{\\theta}(x^{(i)})} = {{\\theta}_{1} x_1^{(i)} + } $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ h_{\\theta}(x^{(i)}) = \\theta_{1} x_1^{(i)} + \\theta_{1} x_1^{(i)} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ h_{\\theta}(x^{(i)}) = \\theta_{1} x_1^{(i)} + \\theta_{2} x_2^{(i)} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ h_{\\theta}(x^{(i)}) = \\theta_{1} x_1^{(i)} + \\theta_{2} x_2^{(i)} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ h_{\\theta}(x^{(i)}) = \\theta_{1} x_1^{(i)} + \\theta_{2} x_2^{(i)} + \\theta_{3} x_3^{(i)} + \\dots + \\theta_{d} x_d^{(i)} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ h_{\\theta}(x^{(i)}) = \\theta_{1} x_1^{(i)} + \\theta_{2} x_2^{(i)} + \\theta_{3} x_3^{(i)} + \\dots + \\theta_{d} x_d^{(i)} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ h_{\\theta}(x^{(i)}) = \\theta_{1} x_1^{(i)} + \\theta_{2} x_2^{(i)} + \\theta_{3} x_3^{(i)} + \\dots + \\theta_{d} x_d^{(i)} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\nx(i) is of the shape (1,d) and θ is of the shape (d,1) (you can see that the shapes are compatible and you can do matrix multiplication)\nhθ(x(i)) is of the shape (1,1) which is nothing but a scalar and it is what you expect because it is the price of a single house.\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ h_{\\theta}(x^{(i)}) = \\theta_{1} x_1^{(i)} + \\theta_{2} x_2^{(i)} + \\theta_{3} x_3^{(i)} + \\dots + \\theta_{d} x_d^{(i)} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\nx^{(i)} is of the shape (1,d) and θ is of the shape (d,1) (you can see that the shapes are compatible and you can do matrix multiplication)\nhθ(x(i)) is of the shape (1,1) which is nothing but a scalar and it is what you expect because it is the price of a single house.\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ h_{\\theta}(x^{(i)}) = \\theta_{1} x_1^{(i)} + \\theta_{2} x_2^{(i)} + \\theta_{3} x_3^{(i)} + \\dots + \\theta_{d} x_d^{(i)} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\nx^{(i)} is of the shape $(1,d)$ and $θ$ is of the shape $(d,1)$ (you can see that the shapes are compatible and you can do matrix multiplication)\nh_θ(x^{(i)}) is of the shape $(1,1)$ which is nothing but a scalar and it is what you expect because it is the price of a single house.\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ h_{\\theta}(x^{(i)}) = \\theta_{1} x_1^{(i)} + \\theta_{2} x_2^{(i)} + \\theta_{3} x_3^{(i)} + \\dots + \\theta_{d} x_d^{(i)} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n$x^{(i)}$ is of the shape $(1,d)$ and $θ$ is of the shape $(d,1)$ (you can see that the shapes are compatible and you can do matrix multiplication)\n$h_θ(x^{(i)})$ is of the shape $(1,1)$ which is nothing but a scalar and it is what you expect because it is the price of a single house.\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ h_{\\theta}(x^{(i)}) = \\theta_{1} x_1^{(i)} + \\theta_{2} x_2^{(i)} + \\theta_{3} x_3^{(i)} + \\dots + \\theta_{d} x_d^{(i)} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n$x^{(i)}$ is of the shape $(1,d)$ and $θ$ is of the shape $(d,1)$ (you can see that the shapes are compatible and you can do matrix multiplication)\n$h_θ(x^{(i)})$ is of the shape $(1,1)$ which is nothing but a scalar and it is what you expect because it is the price of a single house.\nNow, we can stack all the examples (or rows) together and write it in a compact matrix form like,\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ h_{\\theta}(x^{(i)}) = \\theta_{1} x_1^{(i)} + \\theta_{2} x_2^{(i)} + \\theta_{3} x_3^{(i)} + \\dots + \\theta_{d} x_d^{(i)} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n$x^{(i)}$ is of the shape $(1,d)$ and $θ$ is of the shape $(d,1)$ (you can see that the shapes are compatible and you can do matrix multiplication)\n$h_θ(x^{(i)})$ is of the shape $(1,1)$ which is nothing but a scalar and it is what you expect because it is the price of a single house.\nNow, we can stack all the examples (or rows) together and write it in a compact matrix form like,\n$$ \\hat{y} = h_{\\theta}(X) = X \\theta $$\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ h_{\\theta}(x^{(i)}) = \\theta_{1} x_1^{(i)} + \\theta_{2} x_2^{(i)} + \\theta_{3} x_3^{(i)} + \\dots + \\theta_{d} x_d^{(i)} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n$x^{(i)}$ is of the shape $(1,d)$ and $θ$ is of the shape $(d,1)$ (you can see that the shapes are compatible and you can do matrix multiplication)\n$h_θ(x^{(i)})$ is of the shape $(1,1)$ which is nothing but a scalar and it is what you expect because it is the price of a single house.\nNow, we can stack all the examples (or rows) together and write it in a compact matrix form like,\n$$ \\hat{y} = h_{\\theta}(X) = X \\theta $$\n$X$ is of the shape $(n,d)$ and $θ$ is of the shape $(d,1)$.\n$h_θ(X)$ and $\\hat{y}$ are of the shape $(n,1)$ which are the prices of $n$ houses stored in a vector.\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ h_{\\theta}(x^{(i)}) = \\theta_{1} x_1^{(i)} + \\theta_{2} x_2^{(i)} + \\theta_{3} x_3^{(i)} + \\dots + \\theta_{d} x_d^{(i)} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n$x^{(i)}$ is of the shape $(1,d)$ and $θ$ is of the shape $(d,1)$ (you can see that the shapes are compatible and you can do matrix multiplication)\n$h_θ(x^{(i)})$ is of the shape $(1,1)$ which is nothing but a scalar and it is what you expect because it is the price of a single house.\nNow, we can stack all the examples (or rows) together and write it in a compact matrix form like,\n$$ \\hat{y} = h_{\\theta}(X) = X \\theta $$\n$X$ is of the shape $(n,d)$ and $θ$ is of the shape $(d,1)$.\n$h_θ(X)$ and $\\hat{y}$ are of the shape $(n,1)$ which are the prices of $n$ houses stored in a vector.\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"},{"content":"This is a test page! This is an inline equation: $E = mc^2$\nhello!\n$$ \\int_0^\\infty e^{-x} dx = 1 $$\nThis is an inline equation: $a^* = x - b^*$\nThese are block equations:\n$$ a^* = x - b^* $$\n","permalink":"http://localhost:1313/docs/test/","summary":"\u003ch3 id=\"this-is-a-test-page\"\u003eThis is a test page!\u003c/h3\u003e\n\u003cp\u003eThis is an inline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003ehello!\u003c/p\u003e\n\u003cp\u003e$$\n\\int_0^\\infty e^{-x} dx = 1\n$$\u003c/p\u003e\n\u003cp\u003eThis is an inline equation: $a^* = x - b^*$\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\u003cp\u003e$$\na^* = x - b^*\n$$\u003c/p\u003e","title":"Test"},{"content":"I’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\nSo in this blog, I have four main goals:\nDerive the normal equation of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\nJustify the Least Squares Error as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\nExplain the need for gradient descent — why we use it and how it works.\nImplement everything we learn using python and numpy.\nIntroduction Let us start by introducing the problem. Suppose you have a housing dataset (classic example!) consisting of various features related to a house for example, number of bedrooms, proximity to school, house age etc. and also it’s price. Your task is to find the price of a house given a new set of features.\nLet\u0026rsquo;s say we have $n$ such examples, meaning $n$ rows, and each row contains $d$ features (i.e., $d$ columns). We denote this by $X$.\nThe output is a scalar value, representing the price of the house. Since there are $n$ examples, we have $n$ such values, one for each row. We denote this by $y$.\nThus,\nThe shape of $X$ is $(n,d)$. The shape of $y$ is $(n,1)$. Now, let us assume linear relationship between the price of the house and the features of the house. We can express this relationship as a function.\nFor a single example $x$ which is one single row of $X$,\n$$ h_{\\theta}(x^{(i)}) = \\theta_{1} x_1^{(i)} + \\theta_{2} x_2^{(i)} + \\theta_{3} x_3^{(i)} + \\dots + \\theta_{d} x_d^{(i)} $$\nwhere:\n$h_θ(x^{(i)})$ is the price of the house for the ith row of X. $x_1^{(i)},x_2^{(i)},x_3^{(i)} \\dots x_d^{(i)}$ are the features of the house (e.g., number of bedrooms, house age). $θ_1,θ_2,θ_3 \\dots θ_d$​ are the weights (parameters) that determine how much each feature contributes to the price. The above equation can also be written as,\n$$ h_{\\theta}(x^{(i)}) = x^{(i)} {\\theta} $$\n$x^{(i)}$ is of the shape $(1,d)$ and $θ$ is of the shape $(d,1)$ (you can see that the shapes are compatible and you can do matrix multiplication)\n$h_θ(x^{(i)})$ is of the shape $(1,1)$ which is nothing but a scalar and it is what you expect because it is the price of a single house.\nNow, we can stack all the examples (or rows) together and write it in a compact matrix form like,\n$$ \\hat{y} = h_{\\theta}(X) = X \\theta $$\n$X$ is of the shape $(n,d)$ and $θ$ is of the shape $(d,1)$.\n$h_θ(X)$ and $\\hat{y}$ are of the shape $(n,1)$ which are the prices of $n$ houses stored in a vector.\n","permalink":"http://localhost:1313/docs/linear_regression/","summary":"\u003cp\u003eI’m sure many of you must have read about and/or used Linear Regression, but people often remain unclear about it’s internal working.\u003c/p\u003e\n\u003cp\u003eSo in this blog, I have four main goals:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDerive the \u003cstrong\u003enormal equation\u003c/strong\u003e of LR in batch form. Along the way, I’ll go through a bit on how we tackle differentiation of vectors and matrices (matrix calculus) in the context of Machine Learning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJustify the Least Squares Error\u003c/strong\u003e as the loss function for Linear Regression — why it makes sense mathematically and intuitively.\u003c/p\u003e","title":"Linear Regression From Scratch"}]